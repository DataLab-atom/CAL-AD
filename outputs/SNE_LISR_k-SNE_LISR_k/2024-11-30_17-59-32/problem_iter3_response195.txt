from typing import List, Callable
import numpy as np

def search_root(objective_function: callable, x0: np.ndarray, A_list: List[np.ndarray], b_list: List[np.ndarray],
                   tol: float = 1e-6, max_iter: int = 1000, k: int = 5, rho: float = 0.5, M: float = 1.0,
                   grad_clip_threshold: float = 1e6, step_size_decay: float = 0.5, regularization: float = 1e-3) -> np.ndarray:
    """Implements an enhanced LISR-k optimization algorithm with adaptive scaling, gradient clipping, and regularization."""

    n = len(A_list)
    d = x0.shape[0]
    z_list = [x0.copy() for _ in range(n)]
    B_list = [np.eye(d) for _ in range(n)]  # Initialize B_i^0
    B_bar = np.sum(B_list, axis=0)
    B_bar_inv = np.linalg.inv(B_bar + regularization * np.eye(d))  # Regularization added to B_bar_inv initialization
    
    L = np.max(np.linalg.eigvals(A_list[0]))  # Example, assuming all A_i have similar L
    mu = np.min(np.linalg.eigvals(A_list[0]))  # Example, assuming all A_i have similar mu
    if M is None:  # Default to this if M is not provided
        M = (L / mu) ** (3 / 2) / mu  # Third derivative upper bound, example using L and mu

    r0 = np.linalg.norm(x0)  # Initialize r0

    x = x0.copy()
    step_size = 1.0
    for t in range(max_iter):
        i_t = t % n
        omega = compute_omega(t, n, r0, rho, M, L)

        U = greedy_matrix(omega * B_list[i_t], A_list[i_t], k)
        B_new = srk(omega * B_list[i_t], A_list[i_t], U)
        
        V = (omega * B_list[i_t] - A_list[i_t]) @ U
        B_bar_inv = sherman_morrison(B_bar_inv, V, V, U.T @ V) / omega  # Update B_bar_inv

        grad_sum = np.sum([np.dot(A_i, z_i) + b_i for A_i, z_i, b_i in zip(A_list, z_list, b_list)], axis=0)
        grad_norm = np.linalg.norm(grad_sum)
        
        # Gradient clipping to prevent explosion
        if grad_norm > grad_clip_threshold:
            grad_sum = grad_sum / grad_norm * grad_clip_threshold
        
        x_new = B_bar_inv @ grad_sum  # Update x

        # Adaptive step size based on the change in objective function
        f_x = objective_function(x, A_list, b_list)
        f_x_new = objective_function(x_new, A_list, b_list)
        if f_x_new > f_x:
            step_size *= step_size_decay  # Reduce step size if objective function increases

        x_new = x + step_size * (x_new - x)

        if np.linalg.norm(x_new - x) < tol:
            break

        x = x_new.copy()
        z_list[i_t] = x.copy()
        B_list[i_t] = B_new.copy()

        # Early stopping based on gradient norm
        if grad_norm < tol:
            break

    return x
