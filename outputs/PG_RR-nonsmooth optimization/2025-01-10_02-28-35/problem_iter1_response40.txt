```python
import numpy as np
from typing import List, Tuple

def PG_RR_v2(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Enhanced version of the Proximal Gradient with Random Reshuffling (PG-RR) algorithm.
    This version includes adaptive learning rate and momentum for faster convergence.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Initial learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    momentum = np.zeros_like(x)
    beta = 0.9  # Momentum coefficient
    gamma_min = 1e-5  # Minimum learning rate
    gamma_decay = 0.99  # Learning rate decay factor
    
    for epoch in range(num_epochs):
        gamma = max(gamma * gamma_decay, gamma_min)  # Decay learning rate
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            momentum = beta * momentum + (1 - beta) * gradient
            x = soft_thresholding(x - gamma * momentum, gamma * lambda_)
    
    return x
```
