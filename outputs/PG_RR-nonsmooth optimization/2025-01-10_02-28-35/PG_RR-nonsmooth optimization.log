[2025-01-10 02:28:35,418][root][INFO] - Workspace: /root/AEL-P-SNE/outputs/PG_RR-nonsmooth optimization/2025-01-10_02-28-35
[2025-01-10 02:28:35,418][root][INFO] - Project Root: /root/AEL-P-SNE
[2025-01-10 02:28:35,418][root][INFO] - Using LLM: deepseek-coder
[2025-01-10 02:28:35,419][root][INFO] - Using Algorithm: reevo2d
[2025-01-10 02:28:36,582][root][INFO] - Problem: PG_RR
[2025-01-10 02:28:36,582][root][INFO] - Problem description: Find a point \( x^* \) that minimizes the objective function \( f(x) \). The objective function is defined as:$ \\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}} \\| y_i - A_i x \\|_2^2 + \\lambda \\| x \\|_1 $ where \( A_i \) are definited matrices and \( y_i \) are definited vector. The goal is to determine the optimal point \( x^* \) that achieves the minimum value of this function.
[2025-01-10 02:28:36,583][root][INFO] - Functions name: [soft_thresholding,compute_gradient,PG_RR]
[2025-01-10 02:28:36,583][root][INFO] - Evaluating seed function...
[2025-01-10 02:28:36,584][root][INFO] - Seed function code: 
from dataclasses import dataclass
import random
from typing import List
from typing import Tuple
import numpy as np
def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)
def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n
def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x
[2025-01-10 02:28:37,306][root][INFO] - Iteration 0: Running Code 0
[2025-01-10 02:28:37,509][root][INFO] - Iteration 0: Code Run 0 successful!
[2025-01-10 02:29:26,642][root][INFO] - Iteration 0, response_id 0: Objective value: 0.00778998998637781
[2025-01-10 02:29:27,344][root][INFO] - Iteration 0: Elitist: 0.00778998998637781
[2025-01-10 02:29:27,344][root][INFO] - Iteration 0 finished...
[2025-01-10 02:29:27,344][root][INFO] - Best obj: 0.00778998998637781,Best obj func index: 0, Best Code Path: problem_iter0_code0.py
[2025-01-10 02:29:27,345][root][INFO] - Function Evals: 1
[2025-01-10 02:29:27,346][root][INFO] - Initial Population Prompt: 
System Prompt: 
You are an expert-level algorithm engineer. Your task is to design efficient algorithms that can effectively solve optimization problems.
Your response outputs Python code and nothing else. Format your code as a Python code string: "```python ... ```".

User Prompt: 
There is a Markdown document that contains Python code along with relevant explanations. A target function `soft_thresholding` has been selected from this document.
Write a new `soft_thresholding` for problem:
Find a point \( x^* \) that minimizes the objective function \( f(x) \). The objective function is defined as:$ \\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}} \\| y_i - A_i x \\|_2^2 + \\lambda \\| x \\|_1 $ where \( A_i \) are definited matrices and \( y_i \) are definited vector. The goal is to determine the optimal point \( x^* \) that achieves the minimum value of this function.

Function description:
The `soft_thresholding` function is designed to apply soft thresholding to an input vector, which is crucial in scenarios involving L1 regularization, such as in the context of optimization algorithms. It takes two inputs: `x`, a NumPy ndarray representing the input vector, and `threshold`, a floating-point value that specifies the threshold to be applied. The function returns a NumPy ndarray, which is the thresholded vector, obtained by reducing the absolute values of the elements of `x` by the specified threshold and setting negative values to zero, effectively shrinking small values towards zero while preserving the signs of the larger values. This operation helps to enforce sparsity in solutions, making it particularly valuable in regression problems where L1 regularization is employed.

markdown document:
Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.

```python
def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

```

Refer to the format of a trivial design above. Be very creative and give `soft_thresholding_v2`. Output code only and enclose your code with Python code block: ```python ... ```.


[2025-01-10 02:29:32,293][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:32,445][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:32,770][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:32,972][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:33,017][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:33,051][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:33,302][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,091][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,098][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,231][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,367][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,441][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,452][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,498][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,614][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,810][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,879][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,916][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,925][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,954][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:34,979][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,009][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,021][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,033][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,068][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,548][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:35,681][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:36,862][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:37,279][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:37,727][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:37,760][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:37,933][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,007][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,020][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,376][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,482][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,496][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,565][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,772][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,884][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:38,974][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,364][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,381][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,415][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,444][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,523][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,558][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:39,598][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:40,128][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:40,291][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:40,328][root][INFO] - Initial Population Prompt: 
System Prompt: 
You are an expert-level algorithm engineer. Your task is to design efficient algorithms that can effectively solve optimization problems.
Your response outputs Python code and nothing else. Format your code as a Python code string: "```python ... ```".

User Prompt: 
There is a Markdown document that contains Python code along with relevant explanations. A target function `compute_gradient` has been selected from this document.
Write a new `compute_gradient` for problem:
Find a point \( x^* \) that minimizes the objective function \( f(x) \). The objective function is defined as:$ \\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}} \\| y_i - A_i x \\|_2^2 + \\lambda \\| x \\|_1 $ where \( A_i \) are definited matrices and \( y_i \) are definited vector. The goal is to determine the optimal point \( x^* \) that achieves the minimum value of this function.

Function description:
The `compute_gradient` function is designed to compute the gradient of the smooth part of an objective function used in optimization problems. It takes three inputs: `x`, which is a numpy array representing the solution vector; `A`, a list of numpy arrays that are linear transformation matrices; and `y`, a list of observation vectors, which represents the target values. The function calculates the gradient by iterating over the matrices and observation vectors, applying the formula for the gradient of the least squares loss, and averaging the contribution from all observations. The output is a numpy array representing the gradient vector, which can be used for optimization algorithms like Proximal Gradient methods to iteratively update the solution vector `x`.

markdown document:
Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.

```python
def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

```

Refer to the format of a trivial design above. Be very creative and give `compute_gradient_v2`. Output code only and enclose your code with Python code block: ```python ... ```.


[2025-01-10 02:29:46,350][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:46,758][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:47,250][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:47,296][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:47,531][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:47,904][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:47,928][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,016][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,056][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,069][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,228][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,312][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,326][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,346][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,355][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,380][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,422][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,507][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,796][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,867][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:48,922][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,079][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,104][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,113][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,167][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,231][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,357][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,495][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,519][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,579][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:49,752][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:50,044][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:53,761][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:54,440][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:54,835][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:55,045][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:55,132][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:55,171][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:55,526][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:55,876][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,029][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,219][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,329][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,407][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,461][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,922][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,964][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:56,974][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:57,117][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:57,269][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:29:57,310][root][INFO] - Initial Population Prompt: 
System Prompt: 
You are an expert-level algorithm engineer. Your task is to design efficient algorithms that can effectively solve optimization problems.
Your response outputs Python code and nothing else. Format your code as a Python code string: "```python ... ```".

User Prompt: 
There is a Markdown document that contains Python code along with relevant explanations. A target function `PG_RR` has been selected from this document.
Write a new `PG_RR` for problem:
Find a point \( x^* \) that minimizes the objective function \( f(x) \). The objective function is defined as:$ \\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}} \\| y_i - A_i x \\|_2^2 + \\lambda \\| x \\|_1 $ where \( A_i \) are definited matrices and \( y_i \) are definited vector. The goal is to determine the optimal point \( x^* \) that achieves the minimum value of this function.

Function description:
The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) optimization algorithm, designed to minimize a composite objective function that includes a smooth term and an L1 regularization term. Its inputs consist of a list of linear transformation matrices `A`, a list of observation vectors `y`, a regularization intensity `lambda_`, a learning rate `gamma`, the number of training epochs `num_epochs`, and an initial solution vector `initial_x`. The function outputs a tuple containing the optimal solution vector after iteratively updating it through gradient descent and soft thresholding. The purpose of this function is to find a solution that balances the fidelity to the observations with a penalty for complexity (sparsity) in the solution, making it useful in various machine learning and statistical modeling applications.

markdown document:
Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.Below is the implementation of the `PG_RR` algorithm, adhering to the provided optimization goal and coding rules. The code includes comprehensive error handling, efficient performance, and clear documentation. Additionally, it includes type annotations and a test block for verification.

```python
import numpy as np
from typing import List, Tuple

def objective_function(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray], lambda_: float) -> float:
    """
    Compute the combined objective function consisting of a smooth term and a non-smooth term.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.

    Returns:
        float: The value of the objective function.
    """
    smooth_part = sum(np.linalg.norm(A[i] @ x - y[i]) ** 2 for i in range(len(y))) / len(y)
    nonsmooth_part = lambda_ * np.linalg.norm(x, ord=1)
    return smooth_part + nonsmooth_part

def soft_thresholding(x: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply soft thresholding to the input vector.

    Parameters:
        x (np.ndarray): The input vector.
        threshold (float): The threshold value.

    Returns:
        np.ndarray: The thresholded vector.
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def compute_gradient(x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:
    """
    Compute the gradient of the smooth part of the objective function.

    Parameters:
        x (np.ndarray): The solution vector.
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.

    Returns:
        np.ndarray: The gradient vector.
    """
    n = len(y)
    gradient = np.zeros_like(x)
    for i in range(n):
        gradient += 2 * A[i].T @ (A[i] @ x - y[i])
    return gradient / n

def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

if __name__ == "__main__":
    # Test code
    np.random.seed(42)
    
    # Generate synthetic data
    n_samples = 10
    n_features = 784
    A = [np.random.randn(100, n_features) for _ in range(n_samples)]
    y = [np.random.randn(100) for _ in range(n_samples)]
    lambda_ = 0.1
    gamma = 0.01
    num_epochs = 100
    initial_x = np.random.randn(n_features)
    
    # Run PG_RR algorithm
    optimal_x = PG_RR(A, y, lambda_, gamma, num_epochs, initial_x)
    
    # Compute objective function value
    obj_value = objective_function(optimal_x, A, y, lambda_)
    
    print(f"Optimal solution: {optimal_x}")
    print(f"Objective function value: {obj_value}")
```

### Explanation:
1. **Objective Function**: The `objective_function` computes the combined objective function, which includes the smooth term (average squared Euclidean distance) and the non-smooth term (L1 regularization).

2. **Soft Thresholding**: The `soft_thresholding` function applies the soft thresholding operation, which is crucial for handling the L1 regularization term.

3. **Gradient Computation**: The `compute_gradient` function calculates the gradient of the smooth part of the objective function.

4. **PG_RR Algorithm**: The `PG_RR` function implements the Proximal Gradient with Random Reshuffling (PG-RR) algorithm. It iteratively updates the solution vector `x` using the gradient of the smooth part and applies soft thresholding to handle the L1 regularization.

5. **Test Block**: The test block generates synthetic data, runs the `PG_RR` algorithm, and prints the optimal solution and the objective function value.

This implementation ensures that the code is efficient, well-documented, and adheres to the specified optimization goal.

```python
def PG_RR(A: List[np.ndarray], y: List[np.ndarray], lambda_: float, gamma: float, num_epochs: int, initial_x: np.ndarray) -> Tuple[np.ndarray]:
    """
    Run the entry function of the (PG-RR) algorithm.

    Parameters:
        A (List[np.ndarray]): A list of linear transformation matrices.
        y (List[np.ndarray]): A list of observation vectors.
        lambda_ (float): L1 regularization intensity.
        gamma (float): Learning rate (step size).
        num_epochs (int): Number of training cycles.
        initial_x (np.ndarray): Initial solution vector.

    Returns:
        Tuple[np.ndarray]: The last output containing the optimal solution vector.
    """
    x = initial_x.copy()
    n = len(y)
    
    for epoch in range(num_epochs):
        for i in np.random.permutation(n):
            gradient = 2 * A[i].T @ (A[i] @ x - y[i])
            x = soft_thresholding(x - gamma * gradient, gamma * lambda_)
    
    return x

```

Refer to the format of a trivial design above. Be very creative and give `PG_RR_v2`. Output code only and enclose your code with Python code block: ```python ... ```.


[2025-01-10 02:30:05,212][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:05,299][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:05,408][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:05,867][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:05,923][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,093][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,164][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,185][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,238][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,301][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,408][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,516][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,635][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,708][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,826][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,837][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,847][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,861][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,983][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:06,992][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,019][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,068][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,103][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,124][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,484][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,540][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,629][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,702][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,754][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,776][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:07,997][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:08,196][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:13,969][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:14,116][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:14,918][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,034][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,086][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,112][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,152][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,481][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,598][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,630][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,714][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:15,907][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:16,067][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:16,101][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:16,167][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:16,377][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:16,944][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:20,282][httpx][INFO] - HTTP Request: POST https://api.agicto.cn/v1/chat/completions "HTTP/1.1 200 OK"
[2025-01-10 02:30:21,064][root][INFO] - Iteration 1: Running Code 0
[2025-01-10 02:30:21,270][root][INFO] - Iteration 1: Code Run 0 successful!
[2025-01-10 02:30:21,867][root][INFO] - Iteration 1: Running Code 1
[2025-01-10 02:30:22,073][root][INFO] - Iteration 1: Code Run 1 successful!
[2025-01-10 02:30:22,732][root][INFO] - Iteration 1: Running Code 2
[2025-01-10 02:30:22,939][root][INFO] - Iteration 1: Code Run 2 successful!
[2025-01-10 02:30:23,582][root][INFO] - Iteration 1: Running Code 3
[2025-01-10 02:30:23,788][root][INFO] - Iteration 1: Code Run 3 successful!
[2025-01-10 02:30:24,405][root][INFO] - Iteration 1: Running Code 4
[2025-01-10 02:30:24,610][root][INFO] - Iteration 1: Code Run 4 successful!
[2025-01-10 02:30:25,235][root][INFO] - Iteration 1: Running Code 5
[2025-01-10 02:30:25,440][root][INFO] - Iteration 1: Code Run 5 successful!
[2025-01-10 02:30:26,103][root][INFO] - Iteration 1: Running Code 6
[2025-01-10 02:30:26,308][root][INFO] - Iteration 1: Code Run 6 successful!
[2025-01-10 02:30:26,976][root][INFO] - Iteration 1: Running Code 7
[2025-01-10 02:30:27,180][root][INFO] - Iteration 1: Code Run 7 successful!
[2025-01-10 02:30:27,872][root][INFO] - Iteration 1: Running Code 8
[2025-01-10 02:30:28,079][root][INFO] - Iteration 1: Code Run 8 successful!
[2025-01-10 02:30:28,740][root][INFO] - Iteration 1: Running Code 9
[2025-01-10 02:30:28,946][root][INFO] - Iteration 1: Code Run 9 successful!
[2025-01-10 02:30:29,628][root][INFO] - Iteration 1: Running Code 10
[2025-01-10 02:30:29,836][root][INFO] - Iteration 1: Code Run 10 successful!
[2025-01-10 02:30:30,518][root][INFO] - Iteration 1: Running Code 11
[2025-01-10 02:30:30,724][root][INFO] - Iteration 1: Code Run 11 successful!
[2025-01-10 02:30:31,495][root][INFO] - Iteration 1: Running Code 12
[2025-01-10 02:30:31,703][root][INFO] - Iteration 1: Code Run 12 successful!
[2025-01-10 02:30:32,435][root][INFO] - Iteration 1: Running Code 13
[2025-01-10 02:30:32,640][root][INFO] - Iteration 1: Code Run 13 successful!
[2025-01-10 02:30:33,322][root][INFO] - Iteration 1: Running Code 14
[2025-01-10 02:30:33,526][root][INFO] - Iteration 1: Code Run 14 successful!
[2025-01-10 02:30:34,177][root][INFO] - Iteration 1: Running Code 15
[2025-01-10 02:30:34,377][root][INFO] - Iteration 1: Code Run 15 successful!
[2025-01-10 02:30:35,089][root][INFO] - Iteration 1: Running Code 16
[2025-01-10 02:30:35,289][root][INFO] - Iteration 1: Code Run 16 successful!
[2025-01-10 02:30:36,073][root][INFO] - Iteration 1: Running Code 17
[2025-01-10 02:30:36,272][root][INFO] - Iteration 1: Code Run 17 successful!
[2025-01-10 02:30:36,943][root][INFO] - Iteration 1: Running Code 18
[2025-01-10 02:30:37,177][root][INFO] - Iteration 1: Code Run 18 successful!
[2025-01-10 02:30:37,806][root][INFO] - Iteration 1: Running Code 19
[2025-01-10 02:30:38,006][root][INFO] - Iteration 1: Code Run 19 successful!
[2025-01-10 02:30:38,732][root][INFO] - Iteration 1: Running Code 20
[2025-01-10 02:30:38,934][root][INFO] - Iteration 1: Code Run 20 successful!
[2025-01-10 02:30:39,621][root][INFO] - Iteration 1: Running Code 21
[2025-01-10 02:30:39,821][root][INFO] - Iteration 1: Code Run 21 successful!
[2025-01-10 02:30:40,507][root][INFO] - Iteration 1: Running Code 22
[2025-01-10 02:30:40,722][root][INFO] - Iteration 1: Code Run 22 successful!
[2025-01-10 02:30:41,264][root][INFO] - Iteration 1: Running Code 23
[2025-01-10 02:30:41,464][root][INFO] - Iteration 1: Code Run 23 successful!
[2025-01-10 02:30:42,111][root][INFO] - Iteration 1: Running Code 24
[2025-01-10 02:30:42,311][root][INFO] - Iteration 1: Code Run 24 successful!
[2025-01-10 02:30:42,986][root][INFO] - Iteration 1: Running Code 25
[2025-01-10 02:30:43,185][root][INFO] - Iteration 1: Code Run 25 successful!
[2025-01-10 02:30:43,817][root][INFO] - Iteration 1: Running Code 26
[2025-01-10 02:30:44,020][root][INFO] - Iteration 1: Code Run 26 successful!
[2025-01-10 02:30:44,675][root][INFO] - Iteration 1: Running Code 27
[2025-01-10 02:30:44,880][root][INFO] - Iteration 1: Code Run 27 successful!
[2025-01-10 02:30:45,529][root][INFO] - Iteration 1: Running Code 28
[2025-01-10 02:30:45,732][root][INFO] - Iteration 1: Code Run 28 successful!
[2025-01-10 02:30:46,412][root][INFO] - Iteration 1: Running Code 29
[2025-01-10 02:30:46,613][root][INFO] - Iteration 1: Code Run 29 successful!
[2025-01-10 02:30:47,339][root][INFO] - Iteration 1: Running Code 30
[2025-01-10 02:30:47,540][root][INFO] - Iteration 1: Code Run 30 successful!
[2025-01-10 02:30:48,199][root][INFO] - Iteration 1: Running Code 31
[2025-01-10 02:30:48,401][root][INFO] - Iteration 1: Code Run 31 successful!
