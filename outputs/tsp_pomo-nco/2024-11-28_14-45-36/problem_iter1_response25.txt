```python
def search_routine_v2(env: Env, model: Model, episodes: float, batch_size: int = 10, aug_factor: int = 8) -> float:
    """
    Executes an enhanced POMO algorithm for the TSP with additional heuristics.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        episodes: Number of episodes to run.
        batch_size: Size of the problem batch.
        aug_factor: Augmentation factor for data.

    Returns:
        The total distance of the minimum valid solution.
    """
    model.eval()
    model.to(model.device)  # Ensure model is on the correct device

    env.load_problems(batch_size, aug_factor)
    total_reward = 0
    best_solution = float('inf')

    for _ in range(int(episodes)):
        rewards = _run_episode(env, model)
        total_reward += rewards.mean().item()
        current_solution = -rewards.mean().item()
        
        # Apply heuristic: 2-opt local search
        improved_solution = apply_2_opt(env.problems)
        if improved_solution < current_solution:
            current_solution = improved_solution
        
        # Update best solution
        if current_solution < best_solution:
            best_solution = current_solution
        
        torch.cuda.empty_cache()  # Clear GPU cache

    total_distance = best_solution  # Optimization goal is negative reward
    return total_distance

def apply_2_opt(problems: torch.Tensor) -> float:
    """
    Applies the 2-opt heuristic to improve the TSP solution.

    Args:
        problems: The TSP problem instances.

    Returns:
        The improved total distance.
    """
    def distance(a, b):
        return np.linalg.norm(a - b)

    def two_opt_swap(route, i, k):
        new_route = np.concatenate((route[0:i], route[i:k+1][::-1], route[k+1:]))
        return new_route

    def total_distance(route):
        return sum(distance(route[i], route[i-1]) for i in range(1, len(route)))

    route = problems[0].numpy()  # Assuming single problem instance for simplicity
    best_distance = total_distance(route)
    improved = True

    while improved:
        improved = False
        for i in range(1, len(route) - 2):
            for k in range(i + 1, len(route) - 1):
                new_route = two_opt_swap(route, i, k)
                new_distance = total_distance(new_route)
                if new_distance < best_distance:
                    route = new_route
                    best_distance = new_distance
                    improved = True
                    break
            if improved:
                break

    return best_distance
```
