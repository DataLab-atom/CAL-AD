```python
def search_routine_v2(env: Env, model: Model, episodes: float, batch_size: int = 10, aug_factor: int = 8, heuristic_factor: float = 0.1) -> float:
    """
    Executes the POMO algorithm for the TSP with an additional heuristic factor.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        episodes: Number of episodes to run.
        batch_size: Size of the problem batch.
        aug_factor: Augmentation factor for data.
        heuristic_factor: A factor to adjust the heuristic influence on the model's decisions.

    Returns:
        The total distance of the minimum valid solution.
    """
    model.eval()
    model.to(model.device)  # Ensure model is on the correct device

    env.load_problems(batch_size, aug_factor)
    total_reward = 0

    for _ in range(int(episodes)):
        reset_state, _, _ = env.reset()
        model.pre_forward(reset_state)

        done = False
        while not done:
            state, _, _ = env.pre_step()
            selected, logits = model(state)
            
            # Apply heuristic adjustment
            heuristic_adjustment = heuristic_factor * torch.randn_like(logits)
            adjusted_logits = logits + heuristic_adjustment
            selected = torch.argmax(adjusted_logits, dim=-1)

            step_state, reward, done = env.step(selected)
        
        total_reward += reward.mean().item()
        torch.cuda.empty_cache()  # Clear GPU cache

    total_distance = -total_reward / episodes  # Optimization goal is negative reward
    return total_distance
```
