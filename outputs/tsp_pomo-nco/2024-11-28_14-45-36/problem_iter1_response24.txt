```python
def search_routine_v2(env: Env, model: Model, episodes: float, batch_size: int = 10, aug_factor: int = 8, heuristic_factor: float = 0.1) -> float:
    """
    Executes an enhanced POMO algorithm for the TSP with heuristic integration.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        episodes: Number of episodes to run.
        batch_size: Size of the problem batch.
        aug_factor: Augmentation factor for data.
        heuristic_factor: Weight for heuristic-based initial solutions.

    Returns:
        The total distance of the minimum valid solution.
    """
    model.eval()
    model.to(model.device)  # Ensure model is on the correct device

    env.load_problems(batch_size, aug_factor)
    total_reward = 0
    best_solution = float('inf')

    def heuristic_initial_solution(env: Env) -> torch.Tensor:
        """
        Generate an initial solution using a heuristic approach.
        """
        # Example heuristic: Nearest Neighbor
        problems = env.problems
        batch_size, num_nodes, _ = problems.shape
        solution = torch.zeros((batch_size, num_nodes + 1), dtype=torch.long, device=problems.device)
        visited = torch.zeros((batch_size, num_nodes), dtype=torch.bool, device=problems.device)

        for b in range(batch_size):
            current_node = torch.randint(num_nodes, (1,)).item()
            solution[b, 0] = current_node
            visited[b, current_node] = True

            for step in range(1, num_nodes):
                distances = torch.norm(problems[b, current_node] - problems[b], dim=1)
                distances[visited[b]] = float('inf')
                next_node = torch.argmin(distances).item()
                solution[b, step] = next_node
                visited[b, next_node] = True
                current_node = next_node

            solution[b, -1] = solution[b, 0]  # Return to start

        return solution

    for _ in range(int(episodes)):
        # Apply heuristic to get an initial solution
        initial_solution = heuristic_initial_solution(env)
        env.set_initial_solution(initial_solution)

        # Run the model with the initial solution as a starting point
        rewards = _run_episode(env, model)
        total_reward += rewards.mean().item()
        torch.cuda.empty_cache()  # Clear GPU cache

        # Track the best solution found
        current_solution_distance = -rewards.mean().item()
        if current_solution_distance < best_solution:
            best_solution = current_solution_distance

    total_distance = best_solution  # Return the best solution found
    return total_distance
```
