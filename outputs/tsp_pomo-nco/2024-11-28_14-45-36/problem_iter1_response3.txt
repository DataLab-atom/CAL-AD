```python
def search_routine_v2(env: Env, model: Model, episodes: float, batch_size: int = 10, aug_factor: int = 8) -> float:
    """
    Executes an enhanced POMO algorithm for the TSP, incorporating heuristics and multi-threading for efficiency.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        episodes: Number of episodes to run.
        batch_size: Size of the problem batch.
        aug_factor: Augmentation factor for data.

    Returns:
        The total distance of the minimum valid solution.
    """
    import threading
    import queue

    model.eval()
    model.to(model.device)  # Ensure model is on the correct device

    env.load_problems(batch_size, aug_factor)
    total_reward = 0
    reward_queue = queue.Queue()

    def run_episode_thread(env, model, queue):
        rewards = _run_episode(env, model)
        queue.put(rewards.mean().item())
        torch.cuda.empty_cache()  # Clear GPU cache

    threads = []
    for _ in range(int(episodes)):
        thread = threading.Thread(target=run_episode_thread, args=(env, model, reward_queue))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    while not reward_queue.empty():
        total_reward += reward_queue.get()

    total_distance = -total_reward / episodes  # Optimization goal is negative reward
    return total_distance
```
