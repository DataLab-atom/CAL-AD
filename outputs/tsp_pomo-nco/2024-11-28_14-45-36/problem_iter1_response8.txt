```python
def search_routine_v2(env: Env, model: Model, episodes: float, batch_size: int = 10, aug_factor: int = 8) -> float:
    """
    Executes an enhanced POMO algorithm for the TSP, incorporating heuristics and multi-step lookahead.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        episodes: Number of episodes to run.
        batch_size: Size of the problem batch.
        aug_factor: Augmentation factor for data.

    Returns:
        The total distance of the minimum valid solution.
    """
    model.eval()
    model.to(model.device)  # Ensure model is on the correct device

    env.load_problems(batch_size, aug_factor)
    total_reward = 0
    min_distance = float('inf')

    for _ in range(int(episodes)):
        rewards = _run_episode(env, model)
        total_reward += rewards.mean().item()
        current_distance = -rewards.mean().item()

        # Heuristic: Multi-step lookahead to refine the solution
        for _ in range(3):  # Perform 3 refinement steps
            env.reset()
            for _ in range(env.problem_size):
                state, _, _ = env.pre_step()
                selected, _ = model(state)
                step_state, reward, done = env.step(selected)
                if done:
                    break
            refined_distance = -reward.mean().item()
            if refined_distance < current_distance:
                current_distance = refined_distance

        if current_distance < min_distance:
            min_distance = current_distance

        torch.cuda.empty_cache()  # Clear GPU cache

    return min_distance
```
