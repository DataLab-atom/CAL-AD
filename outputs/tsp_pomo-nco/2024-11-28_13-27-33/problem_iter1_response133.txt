```python
def _run_episode_v2(env: Env, model: Model, batch_size: int, aug_factor: int) -> float:
    """
    Runs a single episode of the TSP problem with additional heuristics.

    Args:
        env: The TSP environment.
        model: The pre-trained TSP model.
        batch_size: The batch size for processing problems.
        aug_factor: Augmentation factor for data.

    Returns:
        The minimum travel distance found in the episode.
    """
    env.load_problems(batch_size, aug_factor)
    reset_state, _, _ = env.reset()
    model.pre_forward(reset_state)

    done = False
    total_rewards = []
    while not done:
        state, _, _ = env.pre_step()
        selected, _ = model.forward(state)
        step_state, reward, done = env.step(selected)
        total_rewards.append(reward)

    # Apply heuristic to improve the solution
    improved_rewards = heuristic_improvement(total_rewards)

    return -improved_rewards.min().item()  # reward is negative distance

def heuristic_improvement(rewards: list) -> torch.Tensor:
    """
    Applies a heuristic to improve the TSP solution.

    Args:
        rewards: List of rewards (negative distances) from the model.

    Returns:
        Improved rewards after applying the heuristic.
    """
    # Example heuristic: 2-opt local search
    improved_rewards = torch.stack(rewards)
    for _ in range(10):  # Number of iterations for 2-opt
        for i in range(len(rewards)):
            for j in range(i + 2, len(rewards)):
                # Swap edges (i, i+1) and (j, j+1)
                new_rewards = improved_rewards.clone()
                new_rewards[i:j+1] = torch.flip(new_rewards[i:j+1], [0])
                if new_rewards.sum() < improved_rewards.sum():
                    improved_rewards = new_rewards

    return improved_rewards
```
