{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "np.random.seed(0)  # 固定随机种子以确保结果可重复\n",
    "\n",
    "m = 10  # 数据点的数量\n",
    "n = 784  # 特征数量\n",
    "lambda_ = 1e-5  # 正则化参数\n",
    "\n",
    "true_x = np.array(Image.open('4.jpg'))\n",
    "true_x = true_x.reshape(784)\n",
    "\n",
    "x=true_x+np.random.randn(784)*(2.4e-3)\n",
    "\n",
    "phi=np.random.randn(100,784)\n",
    "O = np.random.randn(n, 100, 784)\n",
    "A = [O[i] for i in range(m)]\n",
    "y = [A[i] @ true_x for i in range(m)]\n",
    "\n",
    "'''\n",
    "Constant error setting:C=3 or 5 and Diminishing errors setting:C=20 or 30 and No error setting:C=0\n",
    "Gaussian vector  e=C*r or C*r**t\n",
    "'''\n",
    "r=np.random.normal(loc=0, scale=1e-3, size=100)\n",
    "\n",
    "print((np.linalg.norm((phi@(x-true_x)))**2)/len(y))  # start error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use by iterations\n",
    "class pg_algorithms:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "        \"\"\"Soft-thresholding operator for L1 norm.\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, A, x, y):\n",
    "        \"\"\"Gradient of the smooth part of the objective function.\"\"\"\n",
    "        grad = np.zeros_like(x)\n",
    "        n = len(y)\n",
    "        for i in range(n):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += (A[i].T @ residual) / n\n",
    "        return grad\n",
    "\n",
    "    def proximal_gradient_descent(self, x, A, y, lambda_, true_x, r, phi_x, max_iter, p, alpha, C): \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0))\n",
    "\n",
    "       \n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "            #y0+=C*r\n",
    "\n",
    "            grad = self.gradient_smooth_part(A, x0, y0)\n",
    "            x_new = self.soft_thresholding(x0 - alpha * grad, alpha * lambda_)\n",
    "            x0 = x_new\n",
    "            \n",
    "            # 检查收敛性\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "        \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "#use by time\n",
    "class pg_algorithms_time:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "        \"\"\"Soft-thresholding operator for L1 norm.\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, A, x, y):\n",
    "        \"\"\"Gradient of the smooth part of the objective function.\"\"\"\n",
    "        grad = np.zeros_like(x)\n",
    "        n = len(y)\n",
    "        for i in range(n):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += (A[i].T @ residual) / n\n",
    "        return grad\n",
    "\n",
    "    def proximal_gradient_descent(self, x, A, y, lambda_, true_x, r, phi_x, max_duration, p, alpha, C): \n",
    "        # 初始化变量\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0))\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            #y0+=C*(r**(iterations+1))\n",
    "            y0 += C*r\n",
    "\n",
    "            grad = self.gradient_smooth_part(A, x0, y0)\n",
    "            x_new = self.soft_thresholding(x0 - alpha * grad, alpha * lambda_)\n",
    "            x0 = x_new\n",
    "            \n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)\n",
    "            iterations += 1\n",
    "\n",
    "            # 检查是否达到下一次打印的时间点\n",
    "            if current_time >= next_print_time:\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                next_print_time += print_interval  # 更新下次打印的时间点\n",
    "            \n",
    "            y0 = y.copy()\n",
    "            current_time = time.time()  # 更新当前时间\n",
    "\n",
    "        return x0, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bpg_algorithms:\n",
    "#use by iterations\n",
    "    def gradient_of_smooth_part(self, x, y, A):\n",
    "        \"\"\"计算平滑部分的梯度\"\"\"\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(y)):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += 2 * A[i].T @ residual\n",
    "        return grad / len(y)\n",
    "\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "        \"\"\"软阈值操作 (用于L1正则化的近端步骤)\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n",
    "\n",
    "    def block_proximal_gradient(self, x, A, y, lambda_, true_x, r, phi_x, max_iter, p, learning_rate, C): \n",
    "        \"\"\"块近端梯度算法\"\"\"\n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y))\n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "            #y0+=C*r\n",
    "\n",
    "            # 计算平滑部分的梯度\n",
    "            grad = self.gradient_of_smooth_part(x0, y0, A)\n",
    "            \n",
    "            # 近似更新（使用梯度下降）\n",
    "            x_new = x0 - learning_rate * grad\n",
    "            \n",
    "            # 应用L1正则化的近端操作\n",
    "            x_new = self.soft_thresholding(x_new, lambda_ * learning_rate)\n",
    "            x0 = x_new\n",
    "            \n",
    "            # 检查收敛性\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "            \n",
    "            y0=y.copy()\n",
    "            \n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "#use by time\n",
    "class bpg_algorithms_time:\n",
    "\n",
    "    def gradient_of_smooth_part(self, x, y, A):\n",
    "        \"\"\"计算平滑部分的梯度\"\"\"\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(y)):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += 2 * A[i].T @ residual\n",
    "        return grad / len(y)\n",
    "\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "        \"\"\"软阈值操作 (用于L1正则化的近端步骤)\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n",
    "\n",
    "    def block_proximal_gradient(self, x, A, y, lambda_, true_x, r, phi_x, max_duration, p, learning_rate, C): \n",
    "        \"\"\"块近端梯度算法\"\"\"\n",
    "\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y))\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            y0+=C*(r**(iterations+1))\n",
    "            #y0 += C*r\n",
    "\n",
    "            # 计算平滑部分的梯度\n",
    "            grad = self.gradient_of_smooth_part(x0, y0, A)\n",
    "            \n",
    "            # 近似更新（使用梯度下降）\n",
    "            x_new = x0 - learning_rate * grad\n",
    "            \n",
    "            # 应用L1正则化的近端操作\n",
    "            x_new = self.soft_thresholding(x_new, lambda_ * learning_rate)\n",
    "            x0 = x_new\n",
    "            \n",
    "            # 检查收敛性\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y)\n",
    "            \n",
    "\n",
    "            # 检查是否达到下一次打印的时间点\n",
    "            if current_time >= next_print_time:\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                next_print_time += print_interval  # 更新下次打印的时间点\n",
    "            \n",
    "            y0 = y.copy()\n",
    "            iterations += 1  # 增加迭代计数器\n",
    "            current_time = time.time()\n",
    "\n",
    "        return x0, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spg_algorithm:\n",
    "\n",
    "    def prox_l1(self, v, lambda_):\n",
    "        \"\"\"Proximal operator for L1 regularization.\"\"\"\n",
    "        return np.sign(v) * np.maximum(np.abs(v) - lambda_, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, x, A, y, idx):\n",
    "        \"\"\"Compute the gradient of the smooth part with respect to a single sample or mini-batch.\"\"\"\n",
    "        return 2 *A[idx].T @ (A[idx] @ x - y[idx]) / len(y)\n",
    "\n",
    "    def run_spg(self, x, A, y, lambda_, true_x, r, phi_x, num_iterations, p, step_size, C): \n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y))\n",
    "        for iteration in range(num_iterations):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "            #y0+=C*r\n",
    "\n",
    "            # Randomly select an index for stochastic gradient estimation\n",
    "            idx = np.random.randint(len(y))\n",
    "            \n",
    "            # Compute the stochastic gradient of the smooth part\n",
    "            grad_smooth = self.gradient_smooth_part(x0, A, y0, idx)\n",
    "            \n",
    "            # Perform a gradient descent step\n",
    "            x_temp = x0 - step_size * grad_smooth\n",
    "            \n",
    "            # Apply the proximal operator corresponding to the non-smooth part\n",
    "            x0 = self.prox_l1(x_temp, step_size * lambda_)\n",
    "            \n",
    "            # 检查收敛性\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "        \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    " # by time\n",
    "class spg_algorithm_time:\n",
    "\n",
    "    def prox_l1(self, v, lambda_):\n",
    "        \"\"\"Proximal operator for L1 regularization.\"\"\"\n",
    "        return np.sign(v) * np.maximum(np.abs(v) - lambda_, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, x, A, y, idx):\n",
    "        \"\"\"Compute the gradient of the smooth part with respect to a single sample or mini-batch.\"\"\"\n",
    "        return 2 * A[idx].T @ (A[idx] @ x - y[idx]) / len(y)\n",
    "\n",
    "    def run_spg(self, x, A, y, lambda_, true_x, r, phi_x, max_duration, p, step_size, C): \n",
    "        \"\"\"Stochastic Proximal Gradient algorithm based on time iterations\"\"\"\n",
    "\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y))\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            y0+=C*(r**(iterations+1))\n",
    "            #y0 += C*r\n",
    "\n",
    "            # Randomly select an index for stochastic gradient estimation\n",
    "            idx = np.random.randint(len(y))\n",
    "            \n",
    "            # Compute the stochastic gradient of the smooth part\n",
    "            grad_smooth = self.gradient_smooth_part(x0, A, y0, idx)\n",
    "            \n",
    "            # Perform a gradient descent step\n",
    "            x_temp = x0 - step_size * grad_smooth\n",
    "            \n",
    "            # Apply the proximal operator corresponding to the non-smooth part\n",
    "            x0 = self.prox_l1(x_temp, step_size * lambda_)\n",
    "            \n",
    "            # 检查收敛性\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y)\n",
    "            \n",
    "\n",
    "            # 检查是否达到下一次打印的时间点\n",
    "            if current_time >= next_print_time:\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                next_print_time += print_interval  # 更新下次打印的时间点\n",
    "            \n",
    "            y0 = y.copy()\n",
    "            iterations += 1  # 增加迭代计数器\n",
    "            current_time = time.time()\n",
    "\n",
    "        return x0, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMN Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import cg\n",
    "import math\n",
    "\n",
    "class admm_algorithm:\n",
    "    \n",
    "    def soft_threshold(self, x, threshold):\n",
    "        \"\"\"Soft-thresholding operator.\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def admm_solver(self, x, A, y, lambda_, true_x, r, phi_x, max_iter, p, rho, C):\n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        m, n = len(y0),len(x0)\n",
    "        # 初始化变量\n",
    "        z = x.copy()\n",
    "        u = np.zeros(n)\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)) \n",
    "\n",
    "        #y0+=C*r\n",
    "        AtA = sum([A[i].T @ A[i] for i in range(m)]) / m + rho * np.eye(n)\n",
    "        b1 = sum([A[i].T @ y0[i] for i in range(m)]) / m\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "\n",
    "            # x-update: 解决一个二次规划问题\n",
    "            b = b1 + rho * (z - u)\n",
    "            x0, _ = cg(AtA, b, x0, maxiter=10, tol=(1e-4/(iteration/2+1)/math.pow(0.999,(iteration+1)/3e2)*0.75 + 1e-9))\n",
    "        \n",
    "            # z-update: 使用软阈值操作\n",
    "            z = self.soft_threshold(x0 + u, lambda_ / rho)\n",
    "\n",
    "            # 对偶变量更新\n",
    "            u = u + x0 - z\n",
    "\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "\n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse.linalg import cg\n",
    "import math\n",
    "\n",
    "class admm_algorithm_time:\n",
    "    \n",
    "    def soft_threshold(self, x, threshold):\n",
    "        \"\"\"Soft-thresholding operator.\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def admm_solver(self, x, A, y, lambda_, true_x, r, phi_x, max_duration, p, rho, C):\n",
    "\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        m, n = len(y0), len(x0)\n",
    "        # 初始化变量\n",
    "        z = x.copy()\n",
    "        u = np.zeros(n)\n",
    "\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)) \n",
    "\n",
    "        y0 += C*r\n",
    "        AtA = sum([A[i].T @ A[i] for i in range(m)]) / m + rho * np.eye(n)\n",
    "        b1 = sum([A[i].T @ y0[i] for i in range(m)]) / m\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            \n",
    "            #y0+=C*(r**(iterations+1))\n",
    "\n",
    "            # x-update: 解决一个二次规划问题\n",
    "            b = b1 + rho * (z - u)\n",
    "            x0, _ = cg(AtA, b, x0, maxiter=10, tol=(1e-4/(iterations/2+1)/math.pow(0.999,(iterations+1)/3e2)*0.75 + 1e-9))\n",
    "        \n",
    "            # z-update: 使用软阈值操作\n",
    "            z = self.soft_threshold(x0 + u, lambda_ / rho)\n",
    "\n",
    "            # 对偶变量更新\n",
    "            u = u + x0 - z\n",
    "\n",
    "            current_err = (np.linalg.norm((phi_x@(x0-true_x)))**2)/len(y0)\n",
    "            \n",
    "\n",
    "            # 检查是否达到下一次打印的时间点\n",
    "            if current_time >= next_print_time:\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                next_print_time += print_interval  # 更新下次打印的时间点\n",
    "            \n",
    "            y0 = y.copy()\n",
    "            iterations += 1  # 增加迭代计数器\n",
    "            current_time = time.time()\n",
    "            \n",
    "        return x0, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGRR Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class pgrr_algorithm:\n",
    "    def soft_thresholding(self, x: np.ndarray, threshold: float) -> np.ndarray:\n",
    "        \n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def compute_gradient(self, x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:\n",
    "        \n",
    "        n = len(y)\n",
    "        gradient = np.zeros_like(x)\n",
    "        for i in range(n):\n",
    "            gradient += 2 * A[i].T @ (A[i] @ x - y[i])\n",
    "        return gradient / n\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, phi_x, num_epochs, p, gamma, C):\n",
    "       \n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        y0=y.copy()\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x-true_x)))**2)/len(y))\n",
    "\n",
    "        for iteration in range(num_epochs):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "            #y0+=C*r\n",
    "\n",
    "            for i in np.random.permutation(n):\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                x = self.soft_thresholding(x - gamma * gradient, gamma * lambda_)\n",
    "            \n",
    "            current_err = (np.linalg.norm((phi_x@(x-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "            \n",
    "            y0=y.copy()\n",
    "        \n",
    "        return x,current_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "class pgrr_algorithm_time:\n",
    "    def soft_thresholding(self, x: np.ndarray, threshold: float) -> np.ndarray:\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def compute_gradient(self, x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:\n",
    "        n = len(y)\n",
    "        gradient = np.zeros_like(x)\n",
    "        for i in range(n):\n",
    "            gradient += 2 * A[i].T @ (A[i] @ x - y[i])\n",
    "        return gradient / n\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, phi_x, max_duration, p, gamma, C):\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        y0 = y.copy()\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x-true_x)))**2)/len(y))\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            \n",
    "            #y0 += C*r\n",
    "            y0+=C*(r**(iterations+1))\n",
    "            # 对样本索引进行随机排列\n",
    "            permuted_indices = np.random.permutation(n)\n",
    "            \n",
    "            for i in permuted_indices:\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                x = self.soft_thresholding(x - gamma * gradient, gamma * lambda_)\n",
    "                \n",
    "                # 检查是否达到下一次打印的时间点（在每次梯度更新后）\n",
    "                \n",
    "                if current_time >= next_print_time:\n",
    "                    current_err = (np.linalg.norm((phi_x@(x-true_x)))**2)/len(y0)\n",
    "                    current_errs.append(current_err)\n",
    "                    print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                    next_print_time += print_interval  # 更新下次打印的时间点\n",
    "                \n",
    "                iterations += 1  # 增加迭代计数器\n",
    "                current_time = time.time()\n",
    "                \n",
    "            y0 = y.copy()\n",
    "\n",
    "        return x, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class ours_algorithms:\n",
    "\n",
    "    def soft_thresholding(self ,x, threshold):\n",
    "       \n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "    \n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, phi_x, num_epochs, p, gamma, C, momentum):\n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        velocity = np.zeros_like(x)\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x-true_x)))**2)/len(y))\n",
    "        \n",
    "        \n",
    "        for iteration in range(num_epochs):\n",
    "\n",
    "            y0+=C*(r**(iteration+1))\n",
    "            #y0+=C*r\n",
    "\n",
    "            for i in np.random.permutation(n):\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                velocity = momentum * velocity + gamma * gradient\n",
    "                x = self.soft_thresholding(x - velocity, gamma * lambda_)\n",
    "        \n",
    "            current_err = (np.linalg.norm((phi_x@(x-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "                \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x,current_errs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class ours_algorithms_time:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, phi_x, max_duration, p, gamma, C, momentum):\n",
    "\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        current_time = start_time\n",
    "        next_print_time = start_time + p  # 下次打印的时间点\n",
    "        print_interval = p  # 打印间隔\n",
    "        iterations = 0  # 迭代计数器\n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        velocity = np.zeros_like(x)\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs = []\n",
    "        current_errs.append((np.linalg.norm((phi_x@(x-true_x)))**2)/len(y))\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  # 循环直到达到最大持续时间\n",
    "            y0+=C*(r**(iterations+1))\n",
    "            #y0 += C*r\n",
    "\n",
    "            permuted_indices = np.random.permutation(n)\n",
    "            \n",
    "            for i in permuted_indices:\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                velocity = momentum * velocity + gamma * gradient\n",
    "                x = self.soft_thresholding(x - velocity, gamma * lambda_)\n",
    "                \n",
    "                # 检查是否达到下一次打印的时间点（在每次梯度更新后）\n",
    "                \n",
    "                if current_time >= next_print_time:\n",
    "                    current_err = (np.linalg.norm((phi_x@(x-true_x)))**2)/len(y0)\n",
    "                    current_errs.append(current_err)\n",
    "                    print(f\"Iteration {iterations}, Objective Value: {current_err}\")\n",
    "                    next_print_time += print_interval  # 更新下次打印的时间点\n",
    "                \n",
    "                iterations += 1  # 增加迭代计数器\n",
    "                current_time = time.time()\n",
    "            y0 = y.copy()\n",
    "\n",
    "        return x, current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG algorithm\n",
    "pg=pg_algorithms()\n",
    "\n",
    "# B-PG algorithm\n",
    "bpg = bpg_algorithms()\n",
    "\n",
    "# SPG algorithm\n",
    "spg = spg_algorithm()\n",
    "\n",
    "# ADMM algorithm\n",
    "admm = admm_algorithm()\n",
    "\n",
    "# PGRR algorithm\n",
    "pgrr=pgrr_algorithm()\n",
    "\n",
    "# ours algorithm\n",
    "ours = ours_algorithms()\n",
    "\n",
    "\n",
    "# PG algorithm\n",
    "pg_time=pg_algorithms_time()\n",
    "\n",
    "# B-PG algorithm\n",
    "bpg_time = bpg_algorithms_time()\n",
    "\n",
    "# SPG algorithm\n",
    "spg_time = spg_algorithm_time()\n",
    "\n",
    "# ADMM algorithm\n",
    "admm_time = admm_algorithm_time()\n",
    "\n",
    "# PGRR algorithm\n",
    "pgrr_time=pgrr_algorithm_time()\n",
    "\n",
    "# ours algorithm\n",
    "ours_time = ours_algorithms_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=0\n",
    "p=5000\n",
    "all_iteration=int(1.45e5+1)\n",
    "\n",
    "pg_x,pg_errors= pg.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 2e-7, C)\n",
    "bpg_x,bpg_errors = bpg.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.1e-7, C)\n",
    "spg_x,spg_errors= spg.run_spg(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.4e-6, C)\n",
    "admm_x,admm_errors = admm.admm_solver(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr.PG_RR(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 6.5e-8, C)\n",
    "ours_x,ours_errors= ours.PG_RR(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 6.5e-8, C, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 55\n",
    "p = 2\n",
    "C = 0\n",
    "\n",
    "\n",
    "pg_x,pg_errors= pg_time.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, max_duration, p, 1.6e-7, C)#\n",
    "bpg_x,bpg_errors = bpg_time.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6e-8, C)#\n",
    "spg_x,spg_errors= spg_time.run_spg(x, A, y, lambda_, true_x, r, phi, max_duration, p, 8.8e-8, C)#\n",
    "admm_x,admm_errors = admm_time.admm_solver(x, A, y, lambda_, true_x, r, phi, max_duration, p, 7e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6.5e-8, C)#\n",
    "ours_x,ours_errors= ours_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6.5e-8, C, 0.9)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#iterations = np.arange(0,all_iteration,p)\n",
    "times = np.arange(0,max_duration,p)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(times, pg_errors, ls = '-.', label='PG')\n",
    "ax.plot(times, bpg_errors, ls = '-', color = 'darkorange', marker = 's', label='B-PG')\n",
    "ax.plot(times, spg_errors, ls = '-', color = 'forestgreen', marker = '^', label='SPG')\n",
    "ax.plot(times, admm_errors, ls = '-', color = 'firebrick', marker = '*', label='ADMM')\n",
    "ax.plot(times, pg_rr_errors, ls = '-', color = 'mediumpurple', marker = 'o', label='PG-RR')\n",
    "ax.plot(times, ours_errors, ls = '-', color = 'dodgerblue', marker = 'h', label='Ours')\n",
    "\n",
    "# 设置科学计数法\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 1))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# 设置y轴从0开始\n",
    "#ax.set_ylim(bottom=0,top=0.06)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "\n",
    "# 移除上边框和右边框\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# 添加图注\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 调整布局以适应科学计数法的标签\n",
    "plt.tight_layout()\n",
    "plt.grid(ls = ':')\n",
    "plt.show()\n",
    "\n",
    "# 保存数组到.npy文件\n",
    "# np.save('./data/name.npy', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diminishing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=50\n",
    "p=2000\n",
    "all_iteration=int(5e4+1)\n",
    "\n",
    "pg_x,pg_errors= pg.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 2e-7, C)\n",
    "bpg_x,bpg_errors = bpg.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.1e-7, C)\n",
    "spg_x,spg_errors= spg.run_spg(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.4e-6, C)\n",
    "admm_x,admm_errors = admm.admm_solver(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr.PG_RR(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 6.5e-8, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "iterations = np.arange(0,all_iteration,p)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(iterations, pg_errors, ls = '-.', label='IPG')\n",
    "ax.plot(iterations, bpg_errors, ls = '-', color = 'darkorange', marker = 's', label='IB-PG')\n",
    "ax.plot(iterations, spg_errors, ls = '-', color = 'forestgreen', marker = '^', label='ISPG')\n",
    "ax.plot(iterations, admm_errors, ls = '-', color = 'firebrick', marker = '*', label='IADMM')\n",
    "ax.plot(iterations, pg_rr_errors, ls = '-', color = 'mediumpurple', marker = 'o', label='IPG-RR')\n",
    "\n",
    "# 设置科学计数法\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 1))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# 设置y轴从0开始\n",
    "ax.set_ylim(bottom=0,top=0.06)\n",
    "\n",
    "\n",
    "# 移除上边框和右边框\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# 添加图注\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 调整布局以适应科学计数法的标签\n",
    "plt.tight_layout()\n",
    "plt.grid(ls = ':')\n",
    "plt.show()\n",
    "\n",
    "#np.save('./data/name.npy', np.array([pg_errors,bpg_errors,spg_errors,admm_errors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 55\n",
    "p = 2\n",
    "C = 50\n",
    "\n",
    "\n",
    "pg_x,pg_errors= pg_time.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, max_duration, p, 2.05e-7, C)#\n",
    "bpg_x,bpg_errors = bpg_time.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, max_duration, p, 0.9e-7, C)#\n",
    "spg_x,spg_errors= spg_time.run_spg(x, A, y, lambda_, true_x, r, phi, max_duration, p, 0.1e-6, C)#\n",
    "admm_x,admm_errors = admm_time.admm_solver(x, A, y, lambda_, true_x, r, phi, max_duration, p, 1.1e6, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 2.8e-8, C)#\n",
    "ours_x,ours_errors= ours_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6.5e-8, C, 0.9)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#iterations = np.arange(0,all_iteration,p)\n",
    "times = np.arange(0,max_duration,p)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(times, pg_errors, ls = '-.', label='PG')\n",
    "ax.plot(times, bpg_errors, ls = '-', color = 'darkorange', marker = 's', label='B-PG')\n",
    "ax.plot(times, spg_errors, ls = '-', color = 'forestgreen', marker = '^', label='SPG')\n",
    "ax.plot(times, admm_errors, ls = '-', color = 'firebrick', marker = '*', label='ADMM')\n",
    "ax.plot(times, pg_rr_errors, ls = '-', color = 'mediumpurple', marker = 'o', label='PG-RR')\n",
    "ax.plot(times, ours_errors, ls = '-', color = 'dodgerblue', marker = 'h', label='Ours')\n",
    "\n",
    "\n",
    "# 设置科学计数法\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 1))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# 设置y轴从0开始\n",
    "\n",
    "#ax.set_ylim(bottom=0,top=0.06)\n",
    "ax.set_yscale('log')\n",
    "#ax.yaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=15))  # 主刻度\n",
    "ax.yaxis.set_minor_locator(ticker.LogLocator(base=10.0, subs='all'))\n",
    "\n",
    "\n",
    "# 移除上边框和右边框\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# 添加图注\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 调整布局以适应科学计数法的标签\n",
    "plt.tight_layout()\n",
    "plt.grid(ls = ':')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# np.save('./data/name.npy', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Constant error setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=3  #5\n",
    "p=5000\n",
    "all_iteration=int(1.45e5+1)\n",
    "\n",
    "pg_x,pg_errors= pg.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 2e-7, C)\n",
    "bpg_x,bpg_errors = bpg.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.1e-7, C)\n",
    "spg_x,spg_errors= spg.run_spg(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1.4e-6, C)\n",
    "admm_x,admm_errors = admm.admm_solver(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 1e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr.PG_RR(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 6.5e-8, C)\n",
    "ours_x,ours_errors= ours.PG_RR(x, A, y, lambda_, true_x, r, phi, all_iteration, p, 6.5e-8, C, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 55\n",
    "p = 2\n",
    "C = 3\n",
    "\n",
    "\n",
    "pg_x,pg_errors= pg_time.proximal_gradient_descent(x, A, y, lambda_, true_x, r, phi, max_duration, p, 0.9e-7, C)#\n",
    "bpg_x,bpg_errors = bpg_time.block_proximal_gradient(x, A, y, lambda_, true_x, r, phi, max_duration, p, 0.3e-7, C)#\n",
    "spg_x,spg_errors= spg_time.run_spg(x, A, y, lambda_, true_x, r, phi, max_duration, p, 0.7e-7, C)#\n",
    "admm_x,admm_errors = admm_time.admm_solver(x, A, y, lambda_, true_x, r, phi, max_duration, p, 1e7, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6e-9, C)#\n",
    "ours_x,ours_errors= ours_time.PG_RR(x, A, y, lambda_, true_x, r, phi, max_duration, p, 6.5e-8, C, 0.9)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#iterations = np.arange(0,all_iteration,p)\n",
    "times = np.arange(0,max_duration,p)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(times, pg_errors, ls = '-.', label='PG')\n",
    "ax.plot(times, bpg_errors, ls = '-', color = 'darkorange', marker = 's', label='B-PG')\n",
    "ax.plot(times, spg_errors, ls = '-', color = 'forestgreen', marker = '^', label='SPG')\n",
    "ax.plot(times, admm_errors, ls = '-', color = 'firebrick', marker = '*', label='ADMM')\n",
    "ax.plot(times, pg_rr_errors, ls = '-', color = 'mediumpurple', marker = 'o', label='PG-RR')\n",
    "ax.plot(times, ours_errors, ls = '-', color = 'dodgerblue', marker = 'h', label='Ours')\n",
    "\n",
    "# 设置科学计数法\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 1))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# 设置y轴从0开始\n",
    "#ax.set_ylim(bottom=0,top=0.06)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 移除上边框和右边框\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# 添加图注\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 调整布局以适应科学计数法的标签\n",
    "plt.tight_layout()\n",
    "plt.grid(ls = ':')\n",
    "plt.show()\n",
    "\n",
    "# np.save('./data/name.npy', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#iterations = np.arange(0,all_iteration,p)\n",
    "times = np.arange(0,max_duration,p)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(times, pg_errors, ls = '-.', label='PG')\n",
    "ax.plot(times, bpg_errors, ls = '-', color = 'darkorange', marker = 's', label='B-PG')\n",
    "ax.plot(times, spg_errors, ls = '-', color = 'forestgreen', marker = '^', label='SPG')\n",
    "ax.plot(times, admm_errors, ls = '-', color = 'firebrick', marker = '*', label='ADMM')\n",
    "ax.plot(times, pg_rr_errors, ls = '-', color = 'mediumpurple', marker = 'o', label='PG-RR')\n",
    "ax.plot(times, ours_errors, ls = '-', color = 'dodgerblue', marker = 'h', label='Ours')\n",
    "\n",
    "# 设置科学计数法\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-1, 1))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# 设置y轴从0开始\n",
    "#ax.set_ylim(bottom=0,top=0.06)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 移除上边框和右边框\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "# 添加图注\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 调整布局以适应科学计数法的标签\n",
    "plt.tight_layout()\n",
    "plt.grid(ls = ':')\n",
    "plt.show()\n",
    "\n",
    "# np.save('./data/name.npy', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
