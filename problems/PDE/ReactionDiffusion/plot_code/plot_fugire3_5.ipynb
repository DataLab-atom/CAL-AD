{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run sequence\n",
    "#### 1. old code or new code(ours) \n",
    "#### 2. The others run sequentially "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple\n",
    "\n",
    "def static_loss(model: nn.Module, f: torch.Tensor, f_bc: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the loss between the model's predictions and the true labels.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        f: Input tensor for the first branch.\n",
    "        f_bc: Input tensor for the second branch.\n",
    "        x: Input tensor for the trunk.\n",
    "        y: True labels.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed loss.\n",
    "    \"\"\"\n",
    "    y_out = model.forward(f, f_bc, x)\n",
    "    loss = ((y_out - y)**2).mean()\n",
    "    return loss\n",
    "\n",
    "def static_forward(model: nn.Module, f: torch.Tensor, f_bc: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform the forward pass through the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        f: Input tensor for the first branch.\n",
    "        f_bc: Input tensor for the second branch.\n",
    "        x: Input tensor for the trunk.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The model's predictions.\n",
    "    \"\"\"\n",
    "    y_br1 = model._branch1(f)\n",
    "    y_br2 = model._branch2(f_bc)\n",
    "    y_br = y_br1 * y_br2\n",
    "\n",
    "    y_tr = model._trunk(x)\n",
    "    y_out = torch.einsum(\"ij,kj->ik\", y_br, y_tr)\n",
    "    return y_out\n",
    "\n",
    "def static_init(model: nn.Module, branch1_dim: List[int], branch2_dim: List[int], trunk_dim: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the model's parameters and architecture.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        branch1_dim: Dimensions for the first branch.\n",
    "        branch2_dim: Dimensions for the second branch.\n",
    "        trunk_dim: Dimensions for the trunk.\n",
    "    \"\"\"\n",
    "    model.z_dim = trunk_dim[-1]\n",
    "\n",
    "    # Build branch net for branch1\n",
    "    modules = []\n",
    "    in_channels = branch1_dim[0]\n",
    "    for h_dim in branch1_dim[1:]:\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        in_channels = h_dim\n",
    "    model._branch1 = nn.Sequential(*modules)\n",
    "\n",
    "    # Build branch net for branch2\n",
    "    modules = []\n",
    "    in_channels = branch2_dim[0]\n",
    "    for h_dim in branch2_dim[1:]:\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        in_channels = h_dim\n",
    "    model._branch2 = nn.Sequential(*modules)\n",
    "\n",
    "    # Build trunk net\n",
    "    modules = []\n",
    "    in_channels = trunk_dim[0]\n",
    "    for h_dim in trunk_dim[1:]:\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        in_channels = h_dim\n",
    "    model._trunk = nn.Sequential(*modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import List\n",
    "import numpy as np\n",
    "def static_forward(model, f, f_bc, x):\n",
    "    \"\"\"\n",
    "    Perform the forward pass through the model with simplified feature interaction.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        f: Input tensor for the first branch.\n",
    "        f_bc: Input tensor for the second branch.\n",
    "        x: Input tensor for the trunk.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The model's predictions.\n",
    "    \"\"\"\n",
    "    # Process inputs through branches\n",
    "    y_br1 = model._branch1(f)\n",
    "    y_br2 = model._branch2(f_bc)\n",
    "    \n",
    "    # Combine branch outputs using element-wise multiplication\n",
    "    y_br = y_br1 * y_br2\n",
    "    \n",
    "    # Process trunk input\n",
    "    y_tr = model._trunk(x)\n",
    "    \n",
    "    # Combine branch and trunk outputs using Einstein summation\n",
    "    y_out = torch.einsum(\"ij,kj->ik\", y_br, y_tr)\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "def static_loss(model: nn.Module, f: torch.Tensor, f_bc: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the loss between the model's predictions and the true labels using a weighted mean squared error.\n",
    "    The weights are dynamically computed based on the magnitude of the true labels to emphasize larger errors.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        f: Input tensor for the first branch.\n",
    "        f_bc: Input tensor for the second branch.\n",
    "        x: Input tensor for the trunk.\n",
    "        y: True labels.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The computed weighted loss.\n",
    "    \"\"\"\n",
    "    y_out = model.forward(f, f_bc, x)\n",
    "    # Compute weights based on the magnitude of the true labels\n",
    "    weights = torch.abs(y) + 1.0  # Add 1 to avoid zero weights\n",
    "    # Compute weighted mean squared error\n",
    "    loss = (weights * (y_out - y)**2).mean()\n",
    "    return loss\n",
    "\n",
    "def static_init(model: nn.Module, branch1_dim: List[int], branch2_dim: List[int], trunk_dim: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the model's parameters and architecture with enhanced flexibility and efficiency.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        branch1_dim: Dimensions for the first branch.\n",
    "        branch2_dim: Dimensions for the second branch.\n",
    "        trunk_dim: Dimensions for the trunk.\n",
    "    \"\"\"\n",
    "    model.z_dim = trunk_dim[-1]\n",
    "\n",
    "    # Build branch net for branch1 with residual connections\n",
    "    modules = []\n",
    "    in_channels = branch1_dim[0]\n",
    "    for i, h_dim in enumerate(branch1_dim[1:]):\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        if i > 0 and h_dim == branch1_dim[i]:  # Add residual connection if dimensions match\n",
    "            modules.append(nn.Linear(h_dim, h_dim))\n",
    "        in_channels = h_dim\n",
    "    model._branch1 = nn.Sequential(*modules)\n",
    "\n",
    "    # Build branch net for branch2 with residual connections\n",
    "    modules = []\n",
    "    in_channels = branch2_dim[0]\n",
    "    for i, h_dim in enumerate(branch2_dim[1:]):\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        if i > 0 and h_dim == branch2_dim[i]:  # Add residual connection if dimensions match\n",
    "            modules.append(nn.Linear(h_dim, h_dim))\n",
    "        in_channels = h_dim\n",
    "    model._branch2 = nn.Sequential(*modules)\n",
    "\n",
    "    # Build trunk net with residual connections\n",
    "    modules = []\n",
    "    in_channels = trunk_dim[0]\n",
    "    for i, h_dim in enumerate(trunk_dim[1:]):\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Linear(in_channels, h_dim),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "        if i > 0 and h_dim == trunk_dim[i]:  # Add residual connection if dimensions match\n",
    "            modules.append(nn.Linear(h_dim, h_dim))\n",
    "        in_channels = h_dim\n",
    "    model._trunk = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_umode = 75\n",
    "num_geomode = 50\n",
    "    # num_geomode = 4\n",
    "if_normal = 0\n",
    "\n",
    "dim_br_u = [num_umode, 300, 300, 200]\n",
    "dim_br_geo = [num_geomode, 200, 200, 200]\n",
    "dim_tr = [3, 200, 200, 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opnn(torch.nn.Module):\n",
    "        def __init__(self, branch1_dim, branch2_dim, trunk_dim):\n",
    "            super(opnn, self).__init__()\n",
    "            static_init(self,branch1_dim, branch2_dim, trunk_dim)\n",
    "\n",
    "        def forward(self, f, f_bc, x):\n",
    "            return static_forward(self,f, f_bc, x)\n",
    "        \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "import math\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pickle as pickle\n",
    "\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or '\\\n",
    "            'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "\n",
    "def static_main(model,epochs,device):\n",
    "    ## hyperparameters\n",
    "    '''\n",
    "    args = ParseArgument()\n",
    "    epochs = args.epochs\n",
    "    device = args.device\n",
    "    save_step = args.save_step\n",
    "    test_model = args.test_model\n",
    "    restart = 0\n",
    "    if test_model == 1:\n",
    "        load_path = \"./CheckPts/model_chkpts.pt\"\n",
    "        test_path = \"../FN_plt_data.mat\"\n",
    "    if restart == 1:\n",
    "        load_path = \"./CheckPts/model_chkpts.pt\"\n",
    "    '''\n",
    "    num_umode = 75\n",
    "    num_geomode = 50\n",
    "    # num_geomode = 4\n",
    "    if_normal = 0\n",
    "\n",
    "    dim_br_u = [num_umode, 300, 300, 200]\n",
    "    dim_br_geo = [num_geomode, 200, 200, 200]\n",
    "    dim_tr = [3, 200, 200, 200]\n",
    "\n",
    "    # dim_br_u = [num_umode, 100, 100, 100]\n",
    "    # dim_br_geo = [num_geomode, 100, 100, 100]\n",
    "    # dim_tr = [3, 100, 100, 100]\n",
    "\n",
    "    num_train = 5400\n",
    "    num_test = 599\n",
    "    num_cases = num_train + num_test\n",
    "    '''\n",
    "    ## create folders\n",
    "    dump_train = './Predictions/Train/'\n",
    "    dump_test = './Predictions/Test/'\n",
    "    model_path = 'CheckPts/model_chkpts.pt'\n",
    "    os.makedirs(dump_train, exist_ok=True)\n",
    "    os.makedirs(dump_test, exist_ok=True)\n",
    "    os.makedirs('CheckPts', exist_ok=True)\n",
    "    '''\n",
    "    ## dataset\n",
    "    # datafile = \"../RD_data.mat\"\n",
    "    # dataset = io.loadmat(datafile)\n",
    "    datafile = \"ReactionDiffusion/dataset/dataset_pod_lddmm.npy\"\n",
    "    dataset = np.load(datafile, allow_pickle=True)\n",
    "\n",
    "    nskip = 2\n",
    "    snap_end = 50\n",
    "    u_data = dataset['u_data'][:, 0:snap_end:nskip]\n",
    "\n",
    "    x_data = dataset['x_ref']\n",
    "\n",
    "    x_mesh = dataset[\"x_lddmm_target\"]\n",
    "    # f_u = dataset[\"u_init\"]\n",
    "    f_u = dataset[\"coeff_u0\"]\n",
    "    f_g = dataset[\"param_data\"]\n",
    "    # f_g = dataset[\"coeff_geo\"]\n",
    "    f_g = dataset[\"coeff_mom\"]\n",
    "    print(f_g.shape)\n",
    "\n",
    "\n",
    "    # for tt in range(0, 2999):\n",
    "    #     fig = plt.figure(constrained_layout=False, figsize=(5, 5))\n",
    "    #     gs = fig.add_gridspec(1, 1)\n",
    "    #     ax = fig.add_subplot(gs[0])\n",
    "    #     h = ax.scatter(x_mesh[tt, :, 0], x_mesh[tt, :, 1], c=u_data[tt, 0])\n",
    "    #     ax.set_aspect(1)\n",
    "    #     plt.colorbar(h)\n",
    "    #     ax.set_title(\"x_traget\")\n",
    "    #     fig.savefig('./target_point/x_traget'+str(tt)+'.png')\n",
    "    #     plt.close()\n",
    "    # exit()\n",
    "\n",
    "    ###############################\n",
    "    ## expand x with time\n",
    "    t = dataset[\"t\"]\n",
    "    t_num = t.shape[1]\n",
    "    tx_ext = np.zeros((t_num, x_data.shape[0], 3))\n",
    "    for i in range(0, t_num):\n",
    "        tx_ext[i, :, 0] = t[0, i]\n",
    "        tx_ext[i, :, 1:3] = x_data\n",
    "    tx_ext = tx_ext[0:snap_end:nskip]\n",
    "    num_snap = tx_ext.shape[0]\n",
    "    num_pts = x_data.shape[0]\n",
    "    tx_ext = tx_ext.reshape(-1, 3)\n",
    "    ###############################\n",
    "\n",
    "    ##########################\n",
    "    ## testing/training dataset\n",
    "    u_train = u_data[:num_train] # start from snapshot 1\n",
    "    f_u_train = f_u[:num_train] # predict based on previous snapshot\n",
    "    f_g_train = f_g[:num_train]\n",
    "\n",
    "    u_test = u_data[num_train:(num_train+num_test)]    \n",
    "    f_u_test = f_u[num_train:(num_train+num_test)]\n",
    "    f_g_test = f_g[num_train:(num_train+num_test)]\n",
    "\n",
    "    ## normalization\n",
    "    if if_normal == 1:\n",
    "        f_u_mean = f_u_train.mean(axis=0)\n",
    "        f_u_std = f_u_train.std(axis=0)\n",
    "        \n",
    "        np.savetxt(\"./f_u_mean_std.txt\", np.concatenate((f_u_mean[:, None], f_u_std[:, None]), axis=1))\n",
    "        \n",
    "        f_u_train = (f_u_train - f_u_mean)/f_u_std\n",
    "        f_u_test = (f_u_test - f_u_mean)/f_u_std\n",
    "\n",
    "        ##\n",
    "        f_g_mean = f_g_train.mean(axis=0)\n",
    "        f_g_std = f_g_train.std(axis=0)\n",
    "        np.savetxt(\"./f_g_mean_std.txt\", np.concatenate((f_g_mean[:, None], f_g_std[:, None]), axis=1))\n",
    "        f_g_train = (f_g_train - f_g_mean)/f_g_std\n",
    "        f_g_test = (f_g_test - f_g_mean)/f_g_std\n",
    "\n",
    "\n",
    "    ## tensor\n",
    "    u_train_tensor = torch.tensor(u_train, dtype=torch.float).to(device)\n",
    "    f_u_train_tensor = torch.tensor(f_u_train, dtype=torch.float).to(device)\n",
    "    f_g_train_tensor = torch.tensor(f_g_train, dtype=torch.float).to(device)\n",
    "\n",
    "    # u_test_tensor = torch.tensor(u_test, dtype=torch.float).to(device)\n",
    "    f_u_test_tensor = torch.tensor(f_u_test, dtype=torch.float).to(device)\n",
    "    f_g_test_tensor = torch.tensor(f_g_test, dtype=torch.float).to(device)\n",
    "    xt_tensor = torch.tensor(tx_ext, dtype=torch.float).to(device)\n",
    "\n",
    "    ## initialization\n",
    "    #model = opnn(dim_br_u, dim_br_geo, dim_tr, num_snap, num_pts).to(device) ## note this is not POD-OPNN. Use POD mode to express functions\n",
    "    model = model.to(device)\n",
    "    model = model.float()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0002)\n",
    "    test_model = 0\n",
    "    if test_model == 0:\n",
    "        train_loss = np.zeros((epochs, 1))\n",
    "        test_loss = np.zeros((epochs, 1))\n",
    "\n",
    "        #if restart == 1:\n",
    "        #    checkpoint = torch.load(load_path)\n",
    "        #    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        ## training\n",
    "        def train(epoch, f_u, f_g, x, y):\n",
    "            model.train()\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model.forward(f_u, f_g, x).reshape(-1,num_snap,num_pts)\n",
    "\n",
    "                loss = ((y_pred - y)**2).mean()\n",
    "                train_loss[epoch, 0] = loss\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        ## Iterations\n",
    "        print('start training...', flush=True)\n",
    "        for epoch in range(0, epochs):\n",
    "            if epoch == 20000:\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "            elif epoch == 90000:\n",
    "                # optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "            # elif epoch == epochs - 1000:\n",
    "            #     optimizer = torch.optim.LBFGS(model.parameters())\n",
    "\n",
    "            ## Training\n",
    "            train(epoch, f_u_train_tensor, f_g_train_tensor, xt_tensor, u_train_tensor)\n",
    "            ## Testing\n",
    "            y_pred = to_numpy(model.forward(f_u_test_tensor, f_g_test_tensor, xt_tensor).reshape(-1,num_snap,num_pts))\n",
    "            loss_tmp = ((y_pred - u_test)**2).mean() # denote u_test as testing cardiac field\n",
    "            test_loss[epoch, 0] = loss_tmp\n",
    "\n",
    "            ## testing error\n",
    "            if epoch%1 == 0:\n",
    "                print(f'Epoch: {epoch}, Train Loss: {train_loss[epoch, 0]:.6f}, Test Loss: {test_loss[epoch, 0]:.6f}', flush=True)\n",
    "                \n",
    "            ## Save model\n",
    "            model_path = 'ReactionDiffusion/plot_data/model.pt'\n",
    "            if (epoch+1)%epochs == 0: \n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, model_path)\n",
    "\n",
    "\n",
    "        ## plot loss function\n",
    "        num_epoch = train_loss.shape[0]\n",
    "        x = np.linspace(1, num_epoch, num_epoch)\n",
    "        \n",
    "        ## plot loss\n",
    "\n",
    "        ## save train\n",
    "        u_train_pred = model.forward(f_u_train_tensor, f_g_train_tensor, xt_tensor).reshape(-1,num_snap,num_pts)\n",
    "        u_pred = to_numpy(u_train_pred)\n",
    "        u_true = u_train\n",
    "        num_t = u_train.shape[1]\n",
    "\n",
    "        ## save test\n",
    "        u_test_pred = model.forward(f_u_test_tensor, f_g_test_tensor, xt_tensor).reshape(-1,num_snap,num_pts)\n",
    "        u_pred = to_numpy(u_test_pred)\n",
    "        u_true = u_test\n",
    "\n",
    "        # dataset_test = io.loadmat(test_path)\n",
    "        num_t = f_u_train.shape[1]\n",
    "\n",
    "        ##############\n",
    "        ## save test\n",
    "        u_test_pred = model.forward(f_u_test_tensor, f_g_test_tensor, xt_tensor).reshape(-1,num_snap,num_pts)\n",
    "        u_pred = to_numpy(u_test_pred)\n",
    "        u_true = u_test\n",
    "\n",
    "        rel_l2 = np.linalg.norm(u_pred - u_true, axis=(1,2))/np.linalg.norm(u_true, axis=(1,2))\n",
    "        #np.savetxt(\"/root/AEL-P-SNE/problems/dimon/data_rd_5/rel_l2_test.txt\", rel_l2)\n",
    "        # print(rel_l2)\n",
    "        print(rel_l2.mean())\n",
    "\n",
    "        return rel_l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 50)\n",
      "start training...\n",
      "Epoch: 0, Train Loss: 0.202086, Test Loss: 0.191006\n",
      "Epoch: 1, Train Loss: 0.188994, Test Loss: 0.178519\n",
      "Epoch: 2, Train Loss: 0.176500, Test Loss: 0.166116\n",
      "Epoch: 3, Train Loss: 0.164127, Test Loss: 0.153814\n",
      "Epoch: 4, Train Loss: 0.151906, Test Loss: 0.142104\n",
      "Epoch: 5, Train Loss: 0.140344, Test Loss: 0.131959\n",
      "Epoch: 6, Train Loss: 0.130434, Test Loss: 0.124667\n",
      "Epoch: 7, Train Loss: 0.123478, Test Loss: 0.120995\n",
      "Epoch: 8, Train Loss: 0.120212, Test Loss: 0.119489\n",
      "Epoch: 9, Train Loss: 0.119093, Test Loss: 0.116813\n",
      "Epoch: 10, Train Loss: 0.116695, Test Loss: 0.111264\n",
      "Epoch: 11, Train Loss: 0.111322, Test Loss: 0.103608\n",
      "Epoch: 12, Train Loss: 0.103798, Test Loss: 0.095438\n",
      "Epoch: 13, Train Loss: 0.095774, Test Loss: 0.087988\n",
      "Epoch: 14, Train Loss: 0.088509, Test Loss: 0.081816\n",
      "Epoch: 15, Train Loss: 0.082561, Test Loss: 0.076983\n",
      "Epoch: 16, Train Loss: 0.077971, Test Loss: 0.073401\n",
      "Epoch: 17, Train Loss: 0.074618, Test Loss: 0.071028\n",
      "Epoch: 18, Train Loss: 0.072432, Test Loss: 0.069812\n",
      "Epoch: 19, Train Loss: 0.071340, Test Loss: 0.069466\n",
      "Epoch: 20, Train Loss: 0.071060, Test Loss: 0.069492\n",
      "Epoch: 21, Train Loss: 0.071107, Test Loss: 0.069532\n",
      "Epoch: 22, Train Loss: 0.071137, Test Loss: 0.069507\n",
      "Epoch: 23, Train Loss: 0.071079, Test Loss: 0.069353\n",
      "Epoch: 24, Train Loss: 0.070880, Test Loss: 0.068871\n",
      "Epoch: 25, Train Loss: 0.070362, Test Loss: 0.067935\n",
      "Epoch: 26, Train Loss: 0.069414, Test Loss: 0.066664\n",
      "Epoch: 27, Train Loss: 0.068158, Test Loss: 0.065354\n",
      "Epoch: 28, Train Loss: 0.066877, Test Loss: 0.064290\n",
      "Epoch: 29, Train Loss: 0.065842, Test Loss: 0.063630\n",
      "Epoch: 30, Train Loss: 0.065200, Test Loss: 0.063376\n",
      "Epoch: 31, Train Loss: 0.064948, Test Loss: 0.063421\n",
      "Epoch: 32, Train Loss: 0.064982, Test Loss: 0.063608\n",
      "Epoch: 33, Train Loss: 0.065151, Test Loss: 0.063792\n",
      "Epoch: 34, Train Loss: 0.065316, Test Loss: 0.063867\n",
      "Epoch: 35, Train Loss: 0.065380, Test Loss: 0.063782\n",
      "Epoch: 36, Train Loss: 0.065293, Test Loss: 0.063532\n",
      "Epoch: 37, Train Loss: 0.065051, Test Loss: 0.063150\n",
      "Epoch: 38, Train Loss: 0.064687, Test Loss: 0.062691\n",
      "Epoch: 39, Train Loss: 0.064254, Test Loss: 0.062219\n",
      "Epoch: 40, Train Loss: 0.063811, Test Loss: 0.061788\n",
      "Epoch: 41, Train Loss: 0.063409, Test Loss: 0.061439\n",
      "Epoch: 42, Train Loss: 0.063084, Test Loss: 0.061185\n",
      "Epoch: 43, Train Loss: 0.062848, Test Loss: 0.061020\n",
      "Epoch: 44, Train Loss: 0.062693, Test Loss: 0.060917\n",
      "Epoch: 45, Train Loss: 0.062593, Test Loss: 0.060841\n",
      "Epoch: 46, Train Loss: 0.062514, Test Loss: 0.060760\n",
      "Epoch: 47, Train Loss: 0.062426, Test Loss: 0.060654\n",
      "Epoch: 48, Train Loss: 0.062311, Test Loss: 0.060517\n",
      "Epoch: 49, Train Loss: 0.062167, Test Loss: 0.060358\n",
      "Epoch: 50, Train Loss: 0.062004, Test Loss: 0.060190\n",
      "Epoch: 51, Train Loss: 0.061833, Test Loss: 0.060026\n",
      "Epoch: 52, Train Loss: 0.061666, Test Loss: 0.059872\n",
      "Epoch: 53, Train Loss: 0.061511, Test Loss: 0.059731\n",
      "Epoch: 54, Train Loss: 0.061369, Test Loss: 0.059602\n",
      "Epoch: 55, Train Loss: 0.061238, Test Loss: 0.059481\n",
      "Epoch: 56, Train Loss: 0.061114, Test Loss: 0.059361\n",
      "Epoch: 57, Train Loss: 0.060993, Test Loss: 0.059238\n",
      "Epoch: 58, Train Loss: 0.060871, Test Loss: 0.059110\n",
      "Epoch: 59, Train Loss: 0.060744, Test Loss: 0.058973\n",
      "Epoch: 60, Train Loss: 0.060613, Test Loss: 0.058829\n",
      "Epoch: 61, Train Loss: 0.060477, Test Loss: 0.058681\n",
      "Epoch: 62, Train Loss: 0.060338, Test Loss: 0.058531\n",
      "Epoch: 63, Train Loss: 0.060200, Test Loss: 0.058385\n",
      "Epoch: 64, Train Loss: 0.060064, Test Loss: 0.058243\n",
      "Epoch: 65, Train Loss: 0.059930, Test Loss: 0.058105\n",
      "Epoch: 66, Train Loss: 0.059798, Test Loss: 0.057968\n",
      "Epoch: 67, Train Loss: 0.059664, Test Loss: 0.057831\n",
      "Epoch: 68, Train Loss: 0.059527, Test Loss: 0.057692\n",
      "Epoch: 69, Train Loss: 0.059387, Test Loss: 0.057551\n",
      "Epoch: 70, Train Loss: 0.059244, Test Loss: 0.057408\n",
      "Epoch: 71, Train Loss: 0.059098, Test Loss: 0.057263\n",
      "Epoch: 72, Train Loss: 0.058950, Test Loss: 0.057116\n",
      "Epoch: 73, Train Loss: 0.058801, Test Loss: 0.056968\n",
      "Epoch: 74, Train Loss: 0.058652, Test Loss: 0.056819\n",
      "Epoch: 75, Train Loss: 0.058503, Test Loss: 0.056667\n",
      "Epoch: 76, Train Loss: 0.058350, Test Loss: 0.056509\n",
      "Epoch: 77, Train Loss: 0.058191, Test Loss: 0.056344\n",
      "Epoch: 78, Train Loss: 0.058026, Test Loss: 0.056172\n",
      "Epoch: 79, Train Loss: 0.057854, Test Loss: 0.055995\n",
      "Epoch: 80, Train Loss: 0.057678, Test Loss: 0.055816\n",
      "Epoch: 81, Train Loss: 0.057500, Test Loss: 0.055633\n",
      "Epoch: 82, Train Loss: 0.057319, Test Loss: 0.055446\n",
      "Epoch: 83, Train Loss: 0.057135, Test Loss: 0.055254\n",
      "Epoch: 84, Train Loss: 0.056946, Test Loss: 0.055055\n",
      "Epoch: 85, Train Loss: 0.056750, Test Loss: 0.054849\n",
      "Epoch: 86, Train Loss: 0.056546, Test Loss: 0.054635\n",
      "Epoch: 87, Train Loss: 0.056334, Test Loss: 0.054415\n",
      "Epoch: 88, Train Loss: 0.056113, Test Loss: 0.054190\n",
      "Epoch: 89, Train Loss: 0.055887, Test Loss: 0.053960\n",
      "Epoch: 90, Train Loss: 0.055656, Test Loss: 0.053724\n",
      "Epoch: 91, Train Loss: 0.055419, Test Loss: 0.053482\n",
      "Epoch: 92, Train Loss: 0.055175, Test Loss: 0.053232\n",
      "Epoch: 93, Train Loss: 0.054923, Test Loss: 0.052973\n",
      "Epoch: 94, Train Loss: 0.054661, Test Loss: 0.052704\n",
      "Epoch: 95, Train Loss: 0.054390, Test Loss: 0.052426\n",
      "Epoch: 96, Train Loss: 0.054109, Test Loss: 0.052139\n",
      "Epoch: 97, Train Loss: 0.053819, Test Loss: 0.051844\n",
      "Epoch: 98, Train Loss: 0.053520, Test Loss: 0.051540\n",
      "Epoch: 99, Train Loss: 0.053214, Test Loss: 0.051227\n",
      "Epoch: 100, Train Loss: 0.052899, Test Loss: 0.050905\n",
      "Epoch: 101, Train Loss: 0.052575, Test Loss: 0.050574\n",
      "Epoch: 102, Train Loss: 0.052241, Test Loss: 0.050237\n",
      "Epoch: 103, Train Loss: 0.051899, Test Loss: 0.049894\n",
      "Epoch: 104, Train Loss: 0.051551, Test Loss: 0.049547\n",
      "Epoch: 105, Train Loss: 0.051197, Test Loss: 0.049196\n",
      "Epoch: 106, Train Loss: 0.050838, Test Loss: 0.048839\n",
      "Epoch: 107, Train Loss: 0.050474, Test Loss: 0.048479\n",
      "Epoch: 108, Train Loss: 0.050104, Test Loss: 0.048118\n",
      "Epoch: 109, Train Loss: 0.049733, Test Loss: 0.047756\n",
      "Epoch: 110, Train Loss: 0.049360, Test Loss: 0.047393\n",
      "Epoch: 111, Train Loss: 0.048987, Test Loss: 0.047028\n",
      "Epoch: 112, Train Loss: 0.048613, Test Loss: 0.046663\n",
      "Epoch: 113, Train Loss: 0.048237, Test Loss: 0.046297\n",
      "Epoch: 114, Train Loss: 0.047861, Test Loss: 0.045930\n",
      "Epoch: 115, Train Loss: 0.047483, Test Loss: 0.045558\n",
      "Epoch: 116, Train Loss: 0.047100, Test Loss: 0.045180\n",
      "Epoch: 117, Train Loss: 0.046710, Test Loss: 0.044792\n",
      "Epoch: 118, Train Loss: 0.046309, Test Loss: 0.044389\n",
      "Epoch: 119, Train Loss: 0.045891, Test Loss: 0.043965\n",
      "Epoch: 120, Train Loss: 0.045453, Test Loss: 0.043513\n",
      "Epoch: 121, Train Loss: 0.044987, Test Loss: 0.043026\n",
      "Epoch: 122, Train Loss: 0.044488, Test Loss: 0.042501\n",
      "Epoch: 123, Train Loss: 0.043950, Test Loss: 0.041931\n",
      "Epoch: 124, Train Loss: 0.043367, Test Loss: 0.041312\n",
      "Epoch: 125, Train Loss: 0.042735, Test Loss: 0.040640\n",
      "Epoch: 126, Train Loss: 0.042050, Test Loss: 0.039912\n",
      "Epoch: 127, Train Loss: 0.041309, Test Loss: 0.039128\n",
      "Epoch: 128, Train Loss: 0.040511, Test Loss: 0.038286\n",
      "Epoch: 129, Train Loss: 0.039656, Test Loss: 0.037389\n",
      "Epoch: 130, Train Loss: 0.038745, Test Loss: 0.036440\n",
      "Epoch: 131, Train Loss: 0.037781, Test Loss: 0.035445\n",
      "Epoch: 132, Train Loss: 0.036770, Test Loss: 0.034411\n",
      "Epoch: 133, Train Loss: 0.035717, Test Loss: 0.033347\n",
      "Epoch: 134, Train Loss: 0.034633, Test Loss: 0.032265\n",
      "Epoch: 135, Train Loss: 0.033530, Test Loss: 0.031181\n",
      "Epoch: 136, Train Loss: 0.032424, Test Loss: 0.030116\n",
      "Epoch: 137, Train Loss: 0.031334, Test Loss: 0.029090\n",
      "Epoch: 138, Train Loss: 0.030282, Test Loss: 0.028131\n",
      "Epoch: 139, Train Loss: 0.029293, Test Loss: 0.027262\n",
      "Epoch: 140, Train Loss: 0.028395, Test Loss: 0.026517\n",
      "Epoch: 141, Train Loss: 0.027617, Test Loss: 0.025948\n",
      "Epoch: 142, Train Loss: 0.027021, Test Loss: 0.026101\n",
      "Epoch: 143, Train Loss: 0.027134, Test Loss: 0.028384\n",
      "Epoch: 144, Train Loss: 0.029440, Test Loss: 0.027011\n",
      "Epoch: 145, Train Loss: 0.028026, Test Loss: 0.025473\n",
      "Epoch: 146, Train Loss: 0.026428, Test Loss: 0.027335\n",
      "Epoch: 147, Train Loss: 0.028306, Test Loss: 0.024935\n",
      "Epoch: 148, Train Loss: 0.025893, Test Loss: 0.027005\n",
      "Epoch: 149, Train Loss: 0.027895, Test Loss: 0.024879\n",
      "Epoch: 150, Train Loss: 0.025799, Test Loss: 0.026536\n",
      "Epoch: 151, Train Loss: 0.027422, Test Loss: 0.024806\n",
      "Epoch: 152, Train Loss: 0.025722, Test Loss: 0.026035\n",
      "Epoch: 153, Train Loss: 0.026923, Test Loss: 0.024751\n",
      "Epoch: 154, Train Loss: 0.025647, Test Loss: 0.025439\n",
      "Epoch: 155, Train Loss: 0.026311, Test Loss: 0.024779\n",
      "Epoch: 156, Train Loss: 0.025666, Test Loss: 0.024737\n",
      "Epoch: 157, Train Loss: 0.025623, Test Loss: 0.024820\n",
      "Epoch: 158, Train Loss: 0.025698, Test Loss: 0.024233\n",
      "Epoch: 159, Train Loss: 0.025113, Test Loss: 0.024660\n",
      "Epoch: 160, Train Loss: 0.025521, Test Loss: 0.024109\n",
      "Epoch: 161, Train Loss: 0.024988, Test Loss: 0.024213\n",
      "Epoch: 162, Train Loss: 0.025095, Test Loss: 0.024196\n",
      "Epoch: 163, Train Loss: 0.025071, Test Loss: 0.023879\n",
      "Epoch: 164, Train Loss: 0.024749, Test Loss: 0.024114\n",
      "Epoch: 165, Train Loss: 0.024971, Test Loss: 0.023847\n",
      "Epoch: 166, Train Loss: 0.024714, Test Loss: 0.023803\n",
      "Epoch: 167, Train Loss: 0.024674, Test Loss: 0.023871\n",
      "Epoch: 168, Train Loss: 0.024739, Test Loss: 0.023630\n",
      "Epoch: 169, Train Loss: 0.024493, Test Loss: 0.023721\n",
      "Epoch: 170, Train Loss: 0.024572, Test Loss: 0.023623\n",
      "Epoch: 171, Train Loss: 0.024474, Test Loss: 0.023488\n",
      "Epoch: 172, Train Loss: 0.024344, Test Loss: 0.023546\n",
      "Epoch: 173, Train Loss: 0.024398, Test Loss: 0.023384\n",
      "Epoch: 174, Train Loss: 0.024232, Test Loss: 0.023378\n",
      "Epoch: 175, Train Loss: 0.024216, Test Loss: 0.023345\n",
      "Epoch: 176, Train Loss: 0.024178, Test Loss: 0.023220\n",
      "Epoch: 177, Train Loss: 0.024055, Test Loss: 0.023238\n",
      "Epoch: 178, Train Loss: 0.024072, Test Loss: 0.023137\n",
      "Epoch: 179, Train Loss: 0.023968, Test Loss: 0.023110\n",
      "Epoch: 180, Train Loss: 0.023932, Test Loss: 0.023086\n",
      "Epoch: 181, Train Loss: 0.023902, Test Loss: 0.022999\n",
      "Epoch: 182, Train Loss: 0.023816, Test Loss: 0.022995\n",
      "Epoch: 183, Train Loss: 0.023810, Test Loss: 0.022925\n",
      "Epoch: 184, Train Loss: 0.023737, Test Loss: 0.022897\n",
      "Epoch: 185, Train Loss: 0.023702, Test Loss: 0.022864\n",
      "Epoch: 186, Train Loss: 0.023664, Test Loss: 0.022802\n",
      "Epoch: 187, Train Loss: 0.023601, Test Loss: 0.022779\n",
      "Epoch: 188, Train Loss: 0.023577, Test Loss: 0.022721\n",
      "Epoch: 189, Train Loss: 0.023514, Test Loss: 0.022690\n",
      "Epoch: 190, Train Loss: 0.023477, Test Loss: 0.022647\n",
      "Epoch: 191, Train Loss: 0.023431, Test Loss: 0.022594\n",
      "Epoch: 192, Train Loss: 0.023377, Test Loss: 0.022561\n",
      "Epoch: 193, Train Loss: 0.023341, Test Loss: 0.022508\n",
      "Epoch: 194, Train Loss: 0.023284, Test Loss: 0.022475\n",
      "Epoch: 195, Train Loss: 0.023245, Test Loss: 0.022429\n",
      "Epoch: 196, Train Loss: 0.023196, Test Loss: 0.022383\n",
      "Epoch: 197, Train Loss: 0.023148, Test Loss: 0.022347\n",
      "Epoch: 198, Train Loss: 0.023108, Test Loss: 0.022300\n",
      "Epoch: 199, Train Loss: 0.023056, Test Loss: 0.022268\n",
      "Epoch: 200, Train Loss: 0.023018, Test Loss: 0.022222\n",
      "Epoch: 201, Train Loss: 0.022969, Test Loss: 0.022182\n",
      "Epoch: 202, Train Loss: 0.022926, Test Loss: 0.022144\n",
      "Epoch: 203, Train Loss: 0.022883, Test Loss: 0.022102\n",
      "Epoch: 204, Train Loss: 0.022835, Test Loss: 0.022066\n",
      "Epoch: 205, Train Loss: 0.022795, Test Loss: 0.022022\n",
      "Epoch: 206, Train Loss: 0.022746, Test Loss: 0.021985\n",
      "Epoch: 207, Train Loss: 0.022704, Test Loss: 0.021943\n",
      "Epoch: 208, Train Loss: 0.022658, Test Loss: 0.021904\n",
      "Epoch: 209, Train Loss: 0.022613, Test Loss: 0.021864\n",
      "Epoch: 210, Train Loss: 0.022568, Test Loss: 0.021822\n",
      "Epoch: 211, Train Loss: 0.022521, Test Loss: 0.021783\n",
      "Epoch: 212, Train Loss: 0.022477, Test Loss: 0.021741\n",
      "Epoch: 213, Train Loss: 0.022430, Test Loss: 0.021701\n",
      "Epoch: 214, Train Loss: 0.022385, Test Loss: 0.021660\n",
      "Epoch: 215, Train Loss: 0.022339, Test Loss: 0.021619\n",
      "Epoch: 216, Train Loss: 0.022293, Test Loss: 0.021578\n",
      "Epoch: 217, Train Loss: 0.022247, Test Loss: 0.021537\n",
      "Epoch: 218, Train Loss: 0.022200, Test Loss: 0.021496\n",
      "Epoch: 219, Train Loss: 0.022154, Test Loss: 0.021454\n",
      "Epoch: 220, Train Loss: 0.022107, Test Loss: 0.021413\n",
      "Epoch: 221, Train Loss: 0.022061, Test Loss: 0.021372\n",
      "Epoch: 222, Train Loss: 0.022014, Test Loss: 0.021330\n",
      "Epoch: 223, Train Loss: 0.021967, Test Loss: 0.021288\n",
      "Epoch: 224, Train Loss: 0.021921, Test Loss: 0.021246\n",
      "Epoch: 225, Train Loss: 0.021873, Test Loss: 0.021205\n",
      "Epoch: 226, Train Loss: 0.021826, Test Loss: 0.021162\n",
      "Epoch: 227, Train Loss: 0.021778, Test Loss: 0.021119\n",
      "Epoch: 228, Train Loss: 0.021730, Test Loss: 0.021077\n",
      "Epoch: 229, Train Loss: 0.021682, Test Loss: 0.021034\n",
      "Epoch: 230, Train Loss: 0.021634, Test Loss: 0.020991\n",
      "Epoch: 231, Train Loss: 0.021585, Test Loss: 0.020948\n",
      "Epoch: 232, Train Loss: 0.021536, Test Loss: 0.020904\n",
      "Epoch: 233, Train Loss: 0.021487, Test Loss: 0.020860\n",
      "Epoch: 234, Train Loss: 0.021438, Test Loss: 0.020817\n",
      "Epoch: 235, Train Loss: 0.021389, Test Loss: 0.020772\n",
      "Epoch: 236, Train Loss: 0.021338, Test Loss: 0.020727\n",
      "Epoch: 237, Train Loss: 0.021288, Test Loss: 0.020682\n",
      "Epoch: 238, Train Loss: 0.021238, Test Loss: 0.020637\n",
      "Epoch: 239, Train Loss: 0.021187, Test Loss: 0.020591\n",
      "Epoch: 240, Train Loss: 0.021136, Test Loss: 0.020545\n",
      "Epoch: 241, Train Loss: 0.021084, Test Loss: 0.020498\n",
      "Epoch: 242, Train Loss: 0.021032, Test Loss: 0.020451\n",
      "Epoch: 243, Train Loss: 0.020979, Test Loss: 0.020403\n",
      "Epoch: 244, Train Loss: 0.020927, Test Loss: 0.020354\n",
      "Epoch: 245, Train Loss: 0.020873, Test Loss: 0.020306\n",
      "Epoch: 246, Train Loss: 0.020820, Test Loss: 0.020256\n",
      "Epoch: 247, Train Loss: 0.020765, Test Loss: 0.020206\n",
      "Epoch: 248, Train Loss: 0.020710, Test Loss: 0.020155\n",
      "Epoch: 249, Train Loss: 0.020655, Test Loss: 0.020104\n",
      "Epoch: 250, Train Loss: 0.020599, Test Loss: 0.020052\n",
      "Epoch: 251, Train Loss: 0.020543, Test Loss: 0.019999\n",
      "Epoch: 252, Train Loss: 0.020486, Test Loss: 0.019946\n",
      "Epoch: 253, Train Loss: 0.020429, Test Loss: 0.019891\n",
      "Epoch: 254, Train Loss: 0.020371, Test Loss: 0.019838\n",
      "Epoch: 255, Train Loss: 0.020314, Test Loss: 0.019783\n",
      "Epoch: 256, Train Loss: 0.020257, Test Loss: 0.019734\n",
      "Epoch: 257, Train Loss: 0.020203, Test Loss: 0.019684\n",
      "Epoch: 258, Train Loss: 0.020153, Test Loss: 0.019641\n",
      "Epoch: 259, Train Loss: 0.020105, Test Loss: 0.019586\n",
      "Epoch: 260, Train Loss: 0.020052, Test Loss: 0.019529\n",
      "Epoch: 261, Train Loss: 0.019989, Test Loss: 0.019457\n",
      "Epoch: 262, Train Loss: 0.019916, Test Loss: 0.019391\n",
      "Epoch: 263, Train Loss: 0.019845, Test Loss: 0.019332\n",
      "Epoch: 264, Train Loss: 0.019784, Test Loss: 0.019282\n",
      "Epoch: 265, Train Loss: 0.019733, Test Loss: 0.019234\n",
      "Epoch: 266, Train Loss: 0.019683, Test Loss: 0.019177\n",
      "Epoch: 267, Train Loss: 0.019626, Test Loss: 0.019117\n",
      "Epoch: 268, Train Loss: 0.019561, Test Loss: 0.019053\n",
      "Epoch: 269, Train Loss: 0.019496, Test Loss: 0.018997\n",
      "Epoch: 270, Train Loss: 0.019437, Test Loss: 0.018946\n",
      "Epoch: 271, Train Loss: 0.019384, Test Loss: 0.018895\n",
      "Epoch: 272, Train Loss: 0.019333, Test Loss: 0.018844\n",
      "Epoch: 273, Train Loss: 0.019279, Test Loss: 0.018788\n",
      "Epoch: 274, Train Loss: 0.019222, Test Loss: 0.018732\n",
      "Epoch: 275, Train Loss: 0.019163, Test Loss: 0.018679\n",
      "Epoch: 276, Train Loss: 0.019108, Test Loss: 0.018629\n",
      "Epoch: 277, Train Loss: 0.019056, Test Loss: 0.018582\n",
      "Epoch: 278, Train Loss: 0.019006, Test Loss: 0.018534\n",
      "Epoch: 279, Train Loss: 0.018956, Test Loss: 0.018485\n",
      "Epoch: 280, Train Loss: 0.018905, Test Loss: 0.018435\n",
      "Epoch: 281, Train Loss: 0.018853, Test Loss: 0.018385\n",
      "Epoch: 282, Train Loss: 0.018801, Test Loss: 0.018337\n",
      "Epoch: 283, Train Loss: 0.018749, Test Loss: 0.018289\n",
      "Epoch: 284, Train Loss: 0.018699, Test Loss: 0.018243\n",
      "Epoch: 285, Train Loss: 0.018651, Test Loss: 0.018197\n",
      "Epoch: 286, Train Loss: 0.018603, Test Loss: 0.018152\n",
      "Epoch: 287, Train Loss: 0.018555, Test Loss: 0.018107\n",
      "Epoch: 288, Train Loss: 0.018508, Test Loss: 0.018061\n",
      "Epoch: 289, Train Loss: 0.018460, Test Loss: 0.018016\n",
      "Epoch: 290, Train Loss: 0.018413, Test Loss: 0.017972\n",
      "Epoch: 291, Train Loss: 0.018366, Test Loss: 0.017928\n",
      "Epoch: 292, Train Loss: 0.018321, Test Loss: 0.017885\n",
      "Epoch: 293, Train Loss: 0.018277, Test Loss: 0.017846\n",
      "Epoch: 294, Train Loss: 0.018236, Test Loss: 0.017809\n",
      "Epoch: 295, Train Loss: 0.018200, Test Loss: 0.017777\n",
      "Epoch: 296, Train Loss: 0.018168, Test Loss: 0.017743\n",
      "Epoch: 297, Train Loss: 0.018134, Test Loss: 0.017699\n",
      "Epoch: 298, Train Loss: 0.018089, Test Loss: 0.017634\n",
      "Epoch: 299, Train Loss: 0.018022, Test Loss: 0.017550\n",
      "Epoch: 300, Train Loss: 0.017932, Test Loss: 0.017461\n",
      "Epoch: 301, Train Loss: 0.017838, Test Loss: 0.017388\n",
      "Epoch: 302, Train Loss: 0.017761, Test Loss: 0.017335\n",
      "Epoch: 303, Train Loss: 0.017708, Test Loss: 0.017292\n",
      "Epoch: 304, Train Loss: 0.017666, Test Loss: 0.017243\n",
      "Epoch: 305, Train Loss: 0.017617, Test Loss: 0.017176\n",
      "Epoch: 306, Train Loss: 0.017548, Test Loss: 0.017094\n",
      "Epoch: 307, Train Loss: 0.017462, Test Loss: 0.017008\n",
      "Epoch: 308, Train Loss: 0.017373, Test Loss: 0.016932\n",
      "Epoch: 309, Train Loss: 0.017295, Test Loss: 0.016866\n",
      "Epoch: 310, Train Loss: 0.017227, Test Loss: 0.016801\n",
      "Epoch: 311, Train Loss: 0.017162, Test Loss: 0.016728\n",
      "Epoch: 312, Train Loss: 0.017087, Test Loss: 0.016643\n",
      "Epoch: 313, Train Loss: 0.017000, Test Loss: 0.016551\n",
      "Epoch: 314, Train Loss: 0.016905, Test Loss: 0.016459\n",
      "Epoch: 315, Train Loss: 0.016810, Test Loss: 0.016371\n",
      "Epoch: 316, Train Loss: 0.016720, Test Loss: 0.016285\n",
      "Epoch: 317, Train Loss: 0.016632, Test Loss: 0.016197\n",
      "Epoch: 318, Train Loss: 0.016542, Test Loss: 0.016101\n",
      "Epoch: 319, Train Loss: 0.016444, Test Loss: 0.016000\n",
      "Epoch: 320, Train Loss: 0.016339, Test Loss: 0.015894\n",
      "Epoch: 321, Train Loss: 0.016230, Test Loss: 0.015786\n",
      "Epoch: 322, Train Loss: 0.016118, Test Loss: 0.015677\n",
      "Epoch: 323, Train Loss: 0.016006, Test Loss: 0.015566\n",
      "Epoch: 324, Train Loss: 0.015892, Test Loss: 0.015453\n",
      "Epoch: 325, Train Loss: 0.015775, Test Loss: 0.015336\n",
      "Epoch: 326, Train Loss: 0.015654, Test Loss: 0.015217\n",
      "Epoch: 327, Train Loss: 0.015530, Test Loss: 0.015096\n",
      "Epoch: 328, Train Loss: 0.015405, Test Loss: 0.014975\n",
      "Epoch: 329, Train Loss: 0.015279, Test Loss: 0.014856\n",
      "Epoch: 330, Train Loss: 0.015156, Test Loss: 0.014745\n",
      "Epoch: 331, Train Loss: 0.015041, Test Loss: 0.014648\n",
      "Epoch: 332, Train Loss: 0.014939, Test Loss: 0.014563\n",
      "Epoch: 333, Train Loss: 0.014852, Test Loss: 0.014477\n",
      "Epoch: 334, Train Loss: 0.014759, Test Loss: 0.014339\n",
      "Epoch: 335, Train Loss: 0.014618, Test Loss: 0.014149\n",
      "Epoch: 336, Train Loss: 0.014419, Test Loss: 0.013959\n",
      "Epoch: 337, Train Loss: 0.014219, Test Loss: 0.013811\n",
      "Epoch: 338, Train Loss: 0.014068, Test Loss: 0.013713\n",
      "Epoch: 339, Train Loss: 0.013959, Test Loss: 0.013613\n",
      "Epoch: 340, Train Loss: 0.013859, Test Loss: 0.013503\n",
      "Epoch: 341, Train Loss: 0.013740, Test Loss: 0.013359\n",
      "Epoch: 342, Train Loss: 0.013592, Test Loss: 0.013202\n",
      "Epoch: 343, Train Loss: 0.013428, Test Loss: 0.013066\n",
      "Epoch: 344, Train Loss: 0.013280, Test Loss: 0.012954\n",
      "Epoch: 345, Train Loss: 0.013169, Test Loss: 0.012883\n",
      "Epoch: 346, Train Loss: 0.013087, Test Loss: 0.012800\n",
      "Epoch: 347, Train Loss: 0.013007, Test Loss: 0.012714\n",
      "Epoch: 348, Train Loss: 0.012908, Test Loss: 0.012596\n",
      "Epoch: 349, Train Loss: 0.012793, Test Loss: 0.012494\n",
      "Epoch: 350, Train Loss: 0.012678, Test Loss: 0.012387\n",
      "Epoch: 351, Train Loss: 0.012572, Test Loss: 0.012295\n",
      "Epoch: 352, Train Loss: 0.012470, Test Loss: 0.012195\n",
      "Epoch: 353, Train Loss: 0.012369, Test Loss: 0.012111\n",
      "Epoch: 354, Train Loss: 0.012280, Test Loss: 0.012073\n",
      "Epoch: 355, Train Loss: 0.012228, Test Loss: 0.012067\n",
      "Epoch: 356, Train Loss: 0.012236, Test Loss: 0.012221\n",
      "Epoch: 357, Train Loss: 0.012333, Test Loss: 0.012324\n",
      "Epoch: 358, Train Loss: 0.012489, Test Loss: 0.012398\n",
      "Epoch: 359, Train Loss: 0.012464, Test Loss: 0.011914\n",
      "Epoch: 360, Train Loss: 0.012086, Test Loss: 0.011672\n",
      "Epoch: 361, Train Loss: 0.011824, Test Loss: 0.011876\n",
      "Epoch: 362, Train Loss: 0.011976, Test Loss: 0.011859\n",
      "Epoch: 363, Train Loss: 0.012024, Test Loss: 0.011622\n",
      "Epoch: 364, Train Loss: 0.011741, Test Loss: 0.011536\n",
      "Epoch: 365, Train Loss: 0.011666, Test Loss: 0.011646\n",
      "Epoch: 366, Train Loss: 0.011803, Test Loss: 0.011566\n",
      "Epoch: 367, Train Loss: 0.011676, Test Loss: 0.011378\n",
      "Epoch: 368, Train Loss: 0.011516, Test Loss: 0.011451\n",
      "Epoch: 369, Train Loss: 0.011607, Test Loss: 0.011475\n",
      "Epoch: 370, Train Loss: 0.011585, Test Loss: 0.011287\n",
      "Epoch: 371, Train Loss: 0.011430, Test Loss: 0.011299\n",
      "Epoch: 372, Train Loss: 0.011455, Test Loss: 0.011357\n",
      "Epoch: 373, Train Loss: 0.011470, Test Loss: 0.011196\n",
      "Epoch: 374, Train Loss: 0.011346, Test Loss: 0.011179\n",
      "Epoch: 375, Train Loss: 0.011328, Test Loss: 0.011233\n",
      "Epoch: 376, Train Loss: 0.011354, Test Loss: 0.011115\n",
      "Epoch: 377, Train Loss: 0.011265, Test Loss: 0.011071\n",
      "Epoch: 378, Train Loss: 0.011220, Test Loss: 0.011112\n",
      "Epoch: 379, Train Loss: 0.011240, Test Loss: 0.011032\n",
      "Epoch: 380, Train Loss: 0.011185, Test Loss: 0.010985\n",
      "Epoch: 381, Train Loss: 0.011135, Test Loss: 0.011013\n",
      "Epoch: 382, Train Loss: 0.011150, Test Loss: 0.010976\n",
      "Epoch: 383, Train Loss: 0.011129, Test Loss: 0.010936\n",
      "Epoch: 384, Train Loss: 0.011092, Test Loss: 0.010970\n",
      "Epoch: 385, Train Loss: 0.011118, Test Loss: 0.010990\n",
      "Epoch: 386, Train Loss: 0.011155, Test Loss: 0.011008\n",
      "Epoch: 387, Train Loss: 0.011173, Test Loss: 0.011053\n",
      "Epoch: 388, Train Loss: 0.011211, Test Loss: 0.011033\n",
      "Epoch: 389, Train Loss: 0.011204, Test Loss: 0.010929\n",
      "Epoch: 390, Train Loss: 0.011089, Test Loss: 0.010800\n",
      "Epoch: 391, Train Loss: 0.010956, Test Loss: 0.010734\n",
      "Epoch: 392, Train Loss: 0.010897, Test Loss: 0.010741\n",
      "Epoch: 393, Train Loss: 0.010900, Test Loss: 0.010765\n",
      "Epoch: 394, Train Loss: 0.010928, Test Loss: 0.010767\n",
      "Epoch: 395, Train Loss: 0.010936, Test Loss: 0.010714\n",
      "Epoch: 396, Train Loss: 0.010881, Test Loss: 0.010646\n",
      "Epoch: 397, Train Loss: 0.010806, Test Loss: 0.010608\n",
      "Epoch: 398, Train Loss: 0.010772, Test Loss: 0.010606\n",
      "Epoch: 399, Train Loss: 0.010773, Test Loss: 0.010613\n",
      "Epoch: 400, Train Loss: 0.010772, Test Loss: 0.010584\n",
      "Epoch: 401, Train Loss: 0.010754, Test Loss: 0.010557\n",
      "Epoch: 402, Train Loss: 0.010721, Test Loss: 0.010526\n",
      "Epoch: 403, Train Loss: 0.010683, Test Loss: 0.010487\n",
      "Epoch: 404, Train Loss: 0.010656, Test Loss: 0.010486\n",
      "Epoch: 405, Train Loss: 0.010645, Test Loss: 0.010474\n",
      "Epoch: 406, Train Loss: 0.010635, Test Loss: 0.010448\n",
      "Epoch: 407, Train Loss: 0.010616, Test Loss: 0.010432\n",
      "Epoch: 408, Train Loss: 0.010593, Test Loss: 0.010412\n",
      "Epoch: 409, Train Loss: 0.010573, Test Loss: 0.010388\n",
      "Epoch: 410, Train Loss: 0.010552, Test Loss: 0.010369\n",
      "Epoch: 411, Train Loss: 0.010530, Test Loss: 0.010352\n",
      "Epoch: 412, Train Loss: 0.010510, Test Loss: 0.010326\n",
      "Epoch: 413, Train Loss: 0.010491, Test Loss: 0.010314\n",
      "Epoch: 414, Train Loss: 0.010471, Test Loss: 0.010293\n",
      "Epoch: 415, Train Loss: 0.010455, Test Loss: 0.010282\n",
      "Epoch: 416, Train Loss: 0.010444, Test Loss: 0.010278\n",
      "Epoch: 417, Train Loss: 0.010434, Test Loss: 0.010261\n",
      "Epoch: 418, Train Loss: 0.010423, Test Loss: 0.010255\n",
      "Epoch: 419, Train Loss: 0.010413, Test Loss: 0.010243\n",
      "Epoch: 420, Train Loss: 0.010402, Test Loss: 0.010230\n",
      "Epoch: 421, Train Loss: 0.010388, Test Loss: 0.010213\n",
      "Epoch: 422, Train Loss: 0.010374, Test Loss: 0.010208\n",
      "Epoch: 423, Train Loss: 0.010364, Test Loss: 0.010199\n",
      "Epoch: 424, Train Loss: 0.010360, Test Loss: 0.010204\n",
      "Epoch: 425, Train Loss: 0.010362, Test Loss: 0.010219\n",
      "Epoch: 426, Train Loss: 0.010376, Test Loss: 0.010238\n",
      "Epoch: 427, Train Loss: 0.010404, Test Loss: 0.010285\n",
      "Epoch: 428, Train Loss: 0.010441, Test Loss: 0.010302\n",
      "Epoch: 429, Train Loss: 0.010471, Test Loss: 0.010304\n",
      "Epoch: 430, Train Loss: 0.010461, Test Loss: 0.010224\n",
      "Epoch: 431, Train Loss: 0.010387, Test Loss: 0.010117\n",
      "Epoch: 432, Train Loss: 0.010271, Test Loss: 0.010021\n",
      "Epoch: 433, Train Loss: 0.010178, Test Loss: 0.009997\n",
      "Epoch: 434, Train Loss: 0.010152, Test Loss: 0.010021\n",
      "Epoch: 435, Train Loss: 0.010174, Test Loss: 0.010038\n",
      "Epoch: 436, Train Loss: 0.010196, Test Loss: 0.010034\n",
      "Epoch: 437, Train Loss: 0.010187, Test Loss: 0.009992\n",
      "Epoch: 438, Train Loss: 0.010151, Test Loss: 0.009958\n",
      "Epoch: 439, Train Loss: 0.010112, Test Loss: 0.009936\n",
      "Epoch: 440, Train Loss: 0.010086, Test Loss: 0.009913\n",
      "Epoch: 441, Train Loss: 0.010068, Test Loss: 0.009900\n",
      "Epoch: 442, Train Loss: 0.010048, Test Loss: 0.009874\n",
      "Epoch: 443, Train Loss: 0.010026, Test Loss: 0.009852\n",
      "Epoch: 444, Train Loss: 0.010005, Test Loss: 0.009842\n",
      "Epoch: 445, Train Loss: 0.009988, Test Loss: 0.009819\n",
      "Epoch: 446, Train Loss: 0.009974, Test Loss: 0.009815\n",
      "Epoch: 447, Train Loss: 0.009959, Test Loss: 0.009794\n",
      "Epoch: 448, Train Loss: 0.009945, Test Loss: 0.009785\n",
      "Epoch: 449, Train Loss: 0.009932, Test Loss: 0.009768\n",
      "Epoch: 450, Train Loss: 0.009914, Test Loss: 0.009742\n",
      "Epoch: 451, Train Loss: 0.009891, Test Loss: 0.009721\n",
      "Epoch: 452, Train Loss: 0.009864, Test Loss: 0.009693\n",
      "Epoch: 453, Train Loss: 0.009842, Test Loss: 0.009689\n",
      "Epoch: 454, Train Loss: 0.009829, Test Loss: 0.009676\n",
      "Epoch: 455, Train Loss: 0.009824, Test Loss: 0.009683\n",
      "Epoch: 456, Train Loss: 0.009821, Test Loss: 0.009667\n",
      "Epoch: 457, Train Loss: 0.009816, Test Loss: 0.009678\n",
      "Epoch: 458, Train Loss: 0.009814, Test Loss: 0.009677\n",
      "Epoch: 459, Train Loss: 0.009826, Test Loss: 0.009730\n",
      "Epoch: 460, Train Loss: 0.009863, Test Loss: 0.009768\n",
      "Epoch: 461, Train Loss: 0.009918, Test Loss: 0.009822\n",
      "Epoch: 462, Train Loss: 0.009953, Test Loss: 0.009777\n",
      "Epoch: 463, Train Loss: 0.009926, Test Loss: 0.009726\n",
      "Epoch: 464, Train Loss: 0.009856, Test Loss: 0.009687\n",
      "Epoch: 465, Train Loss: 0.009828, Test Loss: 0.009749\n",
      "Epoch: 466, Train Loss: 0.009886, Test Loss: 0.009852\n",
      "Epoch: 467, Train Loss: 0.009987, Test Loss: 0.009898\n",
      "Epoch: 468, Train Loss: 0.010039, Test Loss: 0.009844\n",
      "Epoch: 469, Train Loss: 0.009977, Test Loss: 0.009699\n",
      "Epoch: 470, Train Loss: 0.009822, Test Loss: 0.009537\n",
      "Epoch: 471, Train Loss: 0.009684, Test Loss: 0.009531\n",
      "Epoch: 472, Train Loss: 0.009646, Test Loss: 0.009507\n",
      "Epoch: 473, Train Loss: 0.009647, Test Loss: 0.009484\n",
      "Epoch: 474, Train Loss: 0.009614, Test Loss: 0.009496\n",
      "Epoch: 475, Train Loss: 0.009611, Test Loss: 0.009503\n",
      "Epoch: 476, Train Loss: 0.009651, Test Loss: 0.009494\n",
      "Epoch: 477, Train Loss: 0.009606, Test Loss: 0.009352\n",
      "Epoch: 478, Train Loss: 0.009483, Test Loss: 0.009321\n",
      "Epoch: 479, Train Loss: 0.009450, Test Loss: 0.009393\n",
      "Epoch: 480, Train Loss: 0.009503, Test Loss: 0.009361\n",
      "Epoch: 481, Train Loss: 0.009498, Test Loss: 0.009333\n",
      "Epoch: 482, Train Loss: 0.009446, Test Loss: 0.009314\n",
      "Epoch: 483, Train Loss: 0.009434, Test Loss: 0.009298\n",
      "Epoch: 484, Train Loss: 0.009422, Test Loss: 0.009266\n",
      "Epoch: 485, Train Loss: 0.009377, Test Loss: 0.009229\n",
      "Epoch: 486, Train Loss: 0.009350, Test Loss: 0.009237\n",
      "Epoch: 487, Train Loss: 0.009349, Test Loss: 0.009221\n",
      "Epoch: 488, Train Loss: 0.009336, Test Loss: 0.009208\n",
      "Epoch: 489, Train Loss: 0.009321, Test Loss: 0.009202\n",
      "Epoch: 490, Train Loss: 0.009312, Test Loss: 0.009179\n",
      "Epoch: 491, Train Loss: 0.009292, Test Loss: 0.009161\n",
      "Epoch: 492, Train Loss: 0.009268, Test Loss: 0.009143\n",
      "Epoch: 493, Train Loss: 0.009253, Test Loss: 0.009124\n",
      "Epoch: 494, Train Loss: 0.009232, Test Loss: 0.009099\n",
      "Epoch: 495, Train Loss: 0.009204, Test Loss: 0.009082\n",
      "Epoch: 496, Train Loss: 0.009189, Test Loss: 0.009080\n",
      "Epoch: 497, Train Loss: 0.009183, Test Loss: 0.009065\n",
      "Epoch: 498, Train Loss: 0.009168, Test Loss: 0.009044\n",
      "Epoch: 499, Train Loss: 0.009148, Test Loss: 0.009037\n",
      "Epoch: 500, Train Loss: 0.009137, Test Loss: 0.009029\n",
      "Epoch: 501, Train Loss: 0.009131, Test Loss: 0.009023\n",
      "Epoch: 502, Train Loss: 0.009123, Test Loss: 0.009022\n",
      "Epoch: 503, Train Loss: 0.009121, Test Loss: 0.009030\n",
      "Epoch: 504, Train Loss: 0.009127, Test Loss: 0.009043\n",
      "Epoch: 505, Train Loss: 0.009145, Test Loss: 0.009097\n",
      "Epoch: 506, Train Loss: 0.009190, Test Loss: 0.009174\n",
      "Epoch: 507, Train Loss: 0.009283, Test Loss: 0.009344\n",
      "Epoch: 508, Train Loss: 0.009436, Test Loss: 0.009512\n",
      "Epoch: 509, Train Loss: 0.009627, Test Loss: 0.009665\n",
      "Epoch: 510, Train Loss: 0.009759, Test Loss: 0.009549\n",
      "Epoch: 511, Train Loss: 0.009660, Test Loss: 0.009224\n",
      "Epoch: 512, Train Loss: 0.009310, Test Loss: 0.008905\n",
      "Epoch: 513, Train Loss: 0.009000, Test Loss: 0.008908\n",
      "Epoch: 514, Train Loss: 0.009001, Test Loss: 0.009107\n",
      "Epoch: 515, Train Loss: 0.009194, Test Loss: 0.009152\n",
      "Epoch: 516, Train Loss: 0.009256, Test Loss: 0.009008\n",
      "Epoch: 517, Train Loss: 0.009088, Test Loss: 0.008832\n",
      "Epoch: 518, Train Loss: 0.008924, Test Loss: 0.008858\n",
      "Epoch: 519, Train Loss: 0.008949, Test Loss: 0.008963\n",
      "Epoch: 520, Train Loss: 0.009042, Test Loss: 0.008915\n",
      "Epoch: 521, Train Loss: 0.009013, Test Loss: 0.008822\n",
      "Epoch: 522, Train Loss: 0.008898, Test Loss: 0.008770\n",
      "Epoch: 523, Train Loss: 0.008856, Test Loss: 0.008810\n",
      "Epoch: 524, Train Loss: 0.008901, Test Loss: 0.008841\n",
      "Epoch: 525, Train Loss: 0.008914, Test Loss: 0.008759\n",
      "Epoch: 526, Train Loss: 0.008851, Test Loss: 0.008722\n",
      "Epoch: 527, Train Loss: 0.008796, Test Loss: 0.008725\n",
      "Epoch: 528, Train Loss: 0.008803, Test Loss: 0.008733\n",
      "Epoch: 529, Train Loss: 0.008822, Test Loss: 0.008730\n",
      "Epoch: 530, Train Loss: 0.008797, Test Loss: 0.008665\n",
      "Epoch: 531, Train Loss: 0.008750, Test Loss: 0.008657\n",
      "Epoch: 532, Train Loss: 0.008731, Test Loss: 0.008668\n",
      "Epoch: 533, Train Loss: 0.008740, Test Loss: 0.008652\n",
      "Epoch: 534, Train Loss: 0.008736, Test Loss: 0.008641\n",
      "Epoch: 535, Train Loss: 0.008707, Test Loss: 0.008599\n",
      "Epoch: 536, Train Loss: 0.008678, Test Loss: 0.008598\n",
      "Epoch: 537, Train Loss: 0.008670, Test Loss: 0.008604\n",
      "Epoch: 538, Train Loss: 0.008672, Test Loss: 0.008582\n",
      "Epoch: 539, Train Loss: 0.008660, Test Loss: 0.008570\n",
      "Epoch: 540, Train Loss: 0.008635, Test Loss: 0.008542\n",
      "Epoch: 541, Train Loss: 0.008614, Test Loss: 0.008537\n",
      "Epoch: 542, Train Loss: 0.008608, Test Loss: 0.008539\n",
      "Epoch: 543, Train Loss: 0.008605, Test Loss: 0.008520\n",
      "Epoch: 544, Train Loss: 0.008593, Test Loss: 0.008508\n",
      "Epoch: 545, Train Loss: 0.008573, Test Loss: 0.008488\n",
      "Epoch: 546, Train Loss: 0.008557, Test Loss: 0.008478\n",
      "Epoch: 547, Train Loss: 0.008547, Test Loss: 0.008477\n",
      "Epoch: 548, Train Loss: 0.008540, Test Loss: 0.008460\n",
      "Epoch: 549, Train Loss: 0.008530, Test Loss: 0.008453\n",
      "Epoch: 550, Train Loss: 0.008516, Test Loss: 0.008436\n",
      "Epoch: 551, Train Loss: 0.008503, Test Loss: 0.008425\n",
      "Epoch: 552, Train Loss: 0.008491, Test Loss: 0.008417\n",
      "Epoch: 553, Train Loss: 0.008481, Test Loss: 0.008403\n",
      "Epoch: 554, Train Loss: 0.008470, Test Loss: 0.008398\n",
      "Epoch: 555, Train Loss: 0.008460, Test Loss: 0.008383\n",
      "Epoch: 556, Train Loss: 0.008449, Test Loss: 0.008378\n",
      "Epoch: 557, Train Loss: 0.008439, Test Loss: 0.008363\n",
      "Epoch: 558, Train Loss: 0.008429, Test Loss: 0.008356\n",
      "Epoch: 559, Train Loss: 0.008417, Test Loss: 0.008340\n",
      "Epoch: 560, Train Loss: 0.008405, Test Loss: 0.008335\n",
      "Epoch: 561, Train Loss: 0.008395, Test Loss: 0.008320\n",
      "Epoch: 562, Train Loss: 0.008385, Test Loss: 0.008318\n",
      "Epoch: 563, Train Loss: 0.008377, Test Loss: 0.008307\n",
      "Epoch: 564, Train Loss: 0.008370, Test Loss: 0.008305\n",
      "Epoch: 565, Train Loss: 0.008365, Test Loss: 0.008299\n",
      "Epoch: 566, Train Loss: 0.008363, Test Loss: 0.008312\n",
      "Epoch: 567, Train Loss: 0.008369, Test Loss: 0.008321\n",
      "Epoch: 568, Train Loss: 0.008391, Test Loss: 0.008397\n",
      "Epoch: 569, Train Loss: 0.008446, Test Loss: 0.008478\n",
      "Epoch: 570, Train Loss: 0.008564, Test Loss: 0.008741\n",
      "Epoch: 571, Train Loss: 0.008781, Test Loss: 0.008967\n",
      "Epoch: 572, Train Loss: 0.009080, Test Loss: 0.009263\n",
      "Epoch: 573, Train Loss: 0.009302, Test Loss: 0.009038\n",
      "Epoch: 574, Train Loss: 0.009144, Test Loss: 0.008644\n",
      "Epoch: 575, Train Loss: 0.008702, Test Loss: 0.008439\n",
      "Epoch: 576, Train Loss: 0.008479, Test Loss: 0.008522\n",
      "Epoch: 577, Train Loss: 0.008628, Test Loss: 0.008649\n",
      "Epoch: 578, Train Loss: 0.008676, Test Loss: 0.008321\n",
      "Epoch: 579, Train Loss: 0.008404, Test Loss: 0.008228\n",
      "Epoch: 580, Train Loss: 0.008300, Test Loss: 0.008476\n",
      "Epoch: 581, Train Loss: 0.008504, Test Loss: 0.008416\n",
      "Epoch: 582, Train Loss: 0.008518, Test Loss: 0.008241\n",
      "Epoch: 583, Train Loss: 0.008284, Test Loss: 0.008206\n",
      "Epoch: 584, Train Loss: 0.008255, Test Loss: 0.008277\n",
      "Epoch: 585, Train Loss: 0.008370, Test Loss: 0.008272\n",
      "Epoch: 586, Train Loss: 0.008307, Test Loss: 0.008163\n",
      "Epoch: 587, Train Loss: 0.008227, Test Loss: 0.008205\n",
      "Epoch: 588, Train Loss: 0.008282, Test Loss: 0.008219\n",
      "Epoch: 589, Train Loss: 0.008261, Test Loss: 0.008106\n",
      "Epoch: 590, Train Loss: 0.008175, Test Loss: 0.008138\n",
      "Epoch: 591, Train Loss: 0.008201, Test Loss: 0.008178\n",
      "Epoch: 592, Train Loss: 0.008227, Test Loss: 0.008100\n",
      "Epoch: 593, Train Loss: 0.008169, Test Loss: 0.008096\n",
      "Epoch: 594, Train Loss: 0.008155, Test Loss: 0.008120\n",
      "Epoch: 595, Train Loss: 0.008171, Test Loss: 0.008065\n",
      "Epoch: 596, Train Loss: 0.008132, Test Loss: 0.008057\n",
      "Epoch: 597, Train Loss: 0.008116, Test Loss: 0.008088\n",
      "Epoch: 598, Train Loss: 0.008139, Test Loss: 0.008052\n",
      "Epoch: 599, Train Loss: 0.008119, Test Loss: 0.008028\n",
      "Epoch: 600, Train Loss: 0.008085, Test Loss: 0.008039\n",
      "Epoch: 601, Train Loss: 0.008092, Test Loss: 0.008021\n",
      "Epoch: 602, Train Loss: 0.008087, Test Loss: 0.008007\n",
      "Epoch: 603, Train Loss: 0.008062, Test Loss: 0.008007\n",
      "Epoch: 604, Train Loss: 0.008063, Test Loss: 0.008002\n",
      "Epoch: 605, Train Loss: 0.008066, Test Loss: 0.007989\n",
      "Epoch: 606, Train Loss: 0.008044, Test Loss: 0.007972\n",
      "Epoch: 607, Train Loss: 0.008030, Test Loss: 0.007969\n",
      "Epoch: 608, Train Loss: 0.008032, Test Loss: 0.007966\n",
      "Epoch: 609, Train Loss: 0.008021, Test Loss: 0.007946\n",
      "Epoch: 610, Train Loss: 0.008006, Test Loss: 0.007945\n",
      "Epoch: 611, Train Loss: 0.008007, Test Loss: 0.007948\n",
      "Epoch: 612, Train Loss: 0.008004, Test Loss: 0.007926\n",
      "Epoch: 613, Train Loss: 0.007988, Test Loss: 0.007919\n",
      "Epoch: 614, Train Loss: 0.007980, Test Loss: 0.007921\n",
      "Epoch: 615, Train Loss: 0.007978, Test Loss: 0.007905\n",
      "Epoch: 616, Train Loss: 0.007967, Test Loss: 0.007895\n",
      "Epoch: 617, Train Loss: 0.007955, Test Loss: 0.007894\n",
      "Epoch: 618, Train Loss: 0.007952, Test Loss: 0.007882\n",
      "Epoch: 619, Train Loss: 0.007947, Test Loss: 0.007879\n",
      "Epoch: 620, Train Loss: 0.007936, Test Loss: 0.007867\n",
      "Epoch: 621, Train Loss: 0.007929, Test Loss: 0.007863\n",
      "Epoch: 622, Train Loss: 0.007926, Test Loss: 0.007861\n",
      "Epoch: 623, Train Loss: 0.007919, Test Loss: 0.007847\n",
      "Epoch: 624, Train Loss: 0.007910, Test Loss: 0.007844\n",
      "Epoch: 625, Train Loss: 0.007905, Test Loss: 0.007841\n",
      "Epoch: 626, Train Loss: 0.007902, Test Loss: 0.007836\n",
      "Epoch: 627, Train Loss: 0.007897, Test Loss: 0.007830\n",
      "Epoch: 628, Train Loss: 0.007893, Test Loss: 0.007836\n",
      "Epoch: 629, Train Loss: 0.007896, Test Loss: 0.007837\n",
      "Epoch: 630, Train Loss: 0.007904, Test Loss: 0.007865\n",
      "Epoch: 631, Train Loss: 0.007923, Test Loss: 0.007895\n",
      "Epoch: 632, Train Loss: 0.007964, Test Loss: 0.007991\n",
      "Epoch: 633, Train Loss: 0.008049, Test Loss: 0.008131\n",
      "Epoch: 634, Train Loss: 0.008205, Test Loss: 0.008409\n",
      "Epoch: 635, Train Loss: 0.008465, Test Loss: 0.008717\n",
      "Epoch: 636, Train Loss: 0.008803, Test Loss: 0.008989\n",
      "Epoch: 637, Train Loss: 0.009042, Test Loss: 0.008768\n",
      "Epoch: 638, Train Loss: 0.008858, Test Loss: 0.008206\n",
      "Epoch: 639, Train Loss: 0.008263, Test Loss: 0.007772\n",
      "Epoch: 640, Train Loss: 0.007841, Test Loss: 0.007920\n",
      "Epoch: 641, Train Loss: 0.007992, Test Loss: 0.008255\n",
      "Epoch: 642, Train Loss: 0.008316, Test Loss: 0.008161\n",
      "Epoch: 643, Train Loss: 0.008239, Test Loss: 0.007835\n",
      "Epoch: 644, Train Loss: 0.007899, Test Loss: 0.007780\n",
      "Epoch: 645, Train Loss: 0.007847, Test Loss: 0.007977\n",
      "Epoch: 646, Train Loss: 0.008049, Test Loss: 0.007974\n",
      "Epoch: 647, Train Loss: 0.008040, Test Loss: 0.007763\n",
      "Epoch: 648, Train Loss: 0.007835, Test Loss: 0.007753\n",
      "Epoch: 649, Train Loss: 0.007820, Test Loss: 0.007874\n",
      "Epoch: 650, Train Loss: 0.007943, Test Loss: 0.007822\n",
      "Epoch: 651, Train Loss: 0.007891, Test Loss: 0.007699\n",
      "Epoch: 652, Train Loss: 0.007767, Test Loss: 0.007734\n",
      "Epoch: 653, Train Loss: 0.007805, Test Loss: 0.007799\n",
      "Epoch: 654, Train Loss: 0.007866, Test Loss: 0.007716\n",
      "Epoch: 655, Train Loss: 0.007787, Test Loss: 0.007659\n",
      "Epoch: 656, Train Loss: 0.007728, Test Loss: 0.007718\n",
      "Epoch: 657, Train Loss: 0.007784, Test Loss: 0.007720\n",
      "Epoch: 658, Train Loss: 0.007793, Test Loss: 0.007652\n",
      "Epoch: 659, Train Loss: 0.007719, Test Loss: 0.007638\n",
      "Epoch: 660, Train Loss: 0.007707, Test Loss: 0.007678\n",
      "Epoch: 661, Train Loss: 0.007750, Test Loss: 0.007663\n",
      "Epoch: 662, Train Loss: 0.007728, Test Loss: 0.007606\n",
      "Epoch: 663, Train Loss: 0.007678, Test Loss: 0.007619\n",
      "Epoch: 664, Train Loss: 0.007688, Test Loss: 0.007641\n",
      "Epoch: 665, Train Loss: 0.007709, Test Loss: 0.007605\n",
      "Epoch: 666, Train Loss: 0.007678, Test Loss: 0.007581\n",
      "Epoch: 667, Train Loss: 0.007649, Test Loss: 0.007593\n",
      "Epoch: 668, Train Loss: 0.007664, Test Loss: 0.007596\n",
      "Epoch: 669, Train Loss: 0.007668, Test Loss: 0.007569\n",
      "Epoch: 670, Train Loss: 0.007640, Test Loss: 0.007552\n",
      "Epoch: 671, Train Loss: 0.007624, Test Loss: 0.007562\n",
      "Epoch: 672, Train Loss: 0.007634, Test Loss: 0.007559\n",
      "Epoch: 673, Train Loss: 0.007631, Test Loss: 0.007537\n",
      "Epoch: 674, Train Loss: 0.007609, Test Loss: 0.007526\n",
      "Epoch: 675, Train Loss: 0.007599, Test Loss: 0.007531\n",
      "Epoch: 676, Train Loss: 0.007603, Test Loss: 0.007524\n",
      "Epoch: 677, Train Loss: 0.007598, Test Loss: 0.007507\n",
      "Epoch: 678, Train Loss: 0.007581, Test Loss: 0.007500\n",
      "Epoch: 679, Train Loss: 0.007572, Test Loss: 0.007497\n",
      "Epoch: 680, Train Loss: 0.007573, Test Loss: 0.007494\n",
      "Epoch: 681, Train Loss: 0.007567, Test Loss: 0.007480\n",
      "Epoch: 682, Train Loss: 0.007554, Test Loss: 0.007469\n",
      "Epoch: 683, Train Loss: 0.007545, Test Loss: 0.007469\n",
      "Epoch: 684, Train Loss: 0.007542, Test Loss: 0.007462\n",
      "Epoch: 685, Train Loss: 0.007538, Test Loss: 0.007453\n",
      "Epoch: 686, Train Loss: 0.007528, Test Loss: 0.007442\n",
      "Epoch: 687, Train Loss: 0.007518, Test Loss: 0.007436\n",
      "Epoch: 688, Train Loss: 0.007512, Test Loss: 0.007433\n",
      "Epoch: 689, Train Loss: 0.007508, Test Loss: 0.007424\n",
      "Epoch: 690, Train Loss: 0.007501, Test Loss: 0.007415\n",
      "Epoch: 691, Train Loss: 0.007491, Test Loss: 0.007407\n",
      "Epoch: 692, Train Loss: 0.007484, Test Loss: 0.007401\n",
      "Epoch: 693, Train Loss: 0.007478, Test Loss: 0.007395\n",
      "Epoch: 694, Train Loss: 0.007473, Test Loss: 0.007387\n",
      "Epoch: 695, Train Loss: 0.007465, Test Loss: 0.007378\n",
      "Epoch: 696, Train Loss: 0.007456, Test Loss: 0.007371\n",
      "Epoch: 697, Train Loss: 0.007449, Test Loss: 0.007364\n",
      "Epoch: 698, Train Loss: 0.007443, Test Loss: 0.007358\n",
      "Epoch: 699, Train Loss: 0.007437, Test Loss: 0.007350\n",
      "Epoch: 700, Train Loss: 0.007429, Test Loss: 0.007342\n",
      "Epoch: 701, Train Loss: 0.007422, Test Loss: 0.007334\n",
      "Epoch: 702, Train Loss: 0.007414, Test Loss: 0.007328\n",
      "Epoch: 703, Train Loss: 0.007409, Test Loss: 0.007322\n",
      "Epoch: 704, Train Loss: 0.007403, Test Loss: 0.007319\n",
      "Epoch: 705, Train Loss: 0.007400, Test Loss: 0.007317\n",
      "Epoch: 706, Train Loss: 0.007399, Test Loss: 0.007326\n",
      "Epoch: 707, Train Loss: 0.007407, Test Loss: 0.007348\n",
      "Epoch: 708, Train Loss: 0.007432, Test Loss: 0.007412\n",
      "Epoch: 709, Train Loss: 0.007494, Test Loss: 0.007525\n",
      "Epoch: 710, Train Loss: 0.007615, Test Loss: 0.007715\n",
      "Epoch: 711, Train Loss: 0.007802, Test Loss: 0.007872\n",
      "Epoch: 712, Train Loss: 0.007966, Test Loss: 0.007842\n",
      "Epoch: 713, Train Loss: 0.007941, Test Loss: 0.007630\n",
      "Epoch: 714, Train Loss: 0.007700, Test Loss: 0.007435\n",
      "Epoch: 715, Train Loss: 0.007528, Test Loss: 0.007534\n",
      "Epoch: 716, Train Loss: 0.007569, Test Loss: 0.007505\n",
      "Epoch: 717, Train Loss: 0.007610, Test Loss: 0.007467\n",
      "Epoch: 718, Train Loss: 0.007530, Test Loss: 0.007412\n",
      "Epoch: 719, Train Loss: 0.007494, Test Loss: 0.007464\n",
      "Epoch: 720, Train Loss: 0.007533, Test Loss: 0.007394\n",
      "Epoch: 721, Train Loss: 0.007457, Test Loss: 0.007244\n",
      "Epoch: 722, Train Loss: 0.007334, Test Loss: 0.007283\n",
      "Epoch: 723, Train Loss: 0.007369, Test Loss: 0.007345\n",
      "Epoch: 724, Train Loss: 0.007411, Test Loss: 0.007231\n",
      "Epoch: 725, Train Loss: 0.007320, Test Loss: 0.007200\n",
      "Epoch: 726, Train Loss: 0.007288, Test Loss: 0.007284\n",
      "Epoch: 727, Train Loss: 0.007356, Test Loss: 0.007234\n",
      "Epoch: 728, Train Loss: 0.007328, Test Loss: 0.007175\n",
      "Epoch: 729, Train Loss: 0.007255, Test Loss: 0.007180\n",
      "Epoch: 730, Train Loss: 0.007271, Test Loss: 0.007193\n",
      "Epoch: 731, Train Loss: 0.007279, Test Loss: 0.007138\n",
      "Epoch: 732, Train Loss: 0.007229, Test Loss: 0.007134\n",
      "Epoch: 733, Train Loss: 0.007224, Test Loss: 0.007145\n",
      "Epoch: 734, Train Loss: 0.007236, Test Loss: 0.007110\n",
      "Epoch: 735, Train Loss: 0.007202, Test Loss: 0.007103\n",
      "Epoch: 736, Train Loss: 0.007189, Test Loss: 0.007108\n",
      "Epoch: 737, Train Loss: 0.007208, Test Loss: 0.007103\n",
      "Epoch: 738, Train Loss: 0.007188, Test Loss: 0.007060\n",
      "Epoch: 739, Train Loss: 0.007156, Test Loss: 0.007068\n",
      "Epoch: 740, Train Loss: 0.007162, Test Loss: 0.007069\n",
      "Epoch: 741, Train Loss: 0.007161, Test Loss: 0.007042\n",
      "Epoch: 742, Train Loss: 0.007136, Test Loss: 0.007032\n",
      "Epoch: 743, Train Loss: 0.007130, Test Loss: 0.007042\n",
      "Epoch: 744, Train Loss: 0.007132, Test Loss: 0.007018\n",
      "Epoch: 745, Train Loss: 0.007118, Test Loss: 0.007015\n",
      "Epoch: 746, Train Loss: 0.007109, Test Loss: 0.007016\n",
      "Epoch: 747, Train Loss: 0.007113, Test Loss: 0.007011\n",
      "Epoch: 748, Train Loss: 0.007106, Test Loss: 0.006994\n",
      "Epoch: 749, Train Loss: 0.007097, Test Loss: 0.007016\n",
      "Epoch: 750, Train Loss: 0.007106, Test Loss: 0.007013\n",
      "Epoch: 751, Train Loss: 0.007121, Test Loss: 0.007048\n",
      "Epoch: 752, Train Loss: 0.007137, Test Loss: 0.007065\n",
      "Epoch: 753, Train Loss: 0.007176, Test Loss: 0.007163\n",
      "Epoch: 754, Train Loss: 0.007249, Test Loss: 0.007235\n",
      "Epoch: 755, Train Loss: 0.007354, Test Loss: 0.007412\n",
      "Epoch: 756, Train Loss: 0.007487, Test Loss: 0.007500\n",
      "Epoch: 757, Train Loss: 0.007627, Test Loss: 0.007628\n",
      "Epoch: 758, Train Loss: 0.007697, Test Loss: 0.007518\n",
      "Epoch: 759, Train Loss: 0.007637, Test Loss: 0.007379\n",
      "Epoch: 760, Train Loss: 0.007466, Test Loss: 0.007179\n",
      "Epoch: 761, Train Loss: 0.007278, Test Loss: 0.007043\n",
      "Epoch: 762, Train Loss: 0.007157, Test Loss: 0.007033\n",
      "Epoch: 763, Train Loss: 0.007115, Test Loss: 0.006997\n",
      "Epoch: 764, Train Loss: 0.007112, Test Loss: 0.007023\n",
      "Epoch: 765, Train Loss: 0.007112, Test Loss: 0.007009\n",
      "Epoch: 766, Train Loss: 0.007113, Test Loss: 0.006992\n",
      "Epoch: 767, Train Loss: 0.007104, Test Loss: 0.006966\n",
      "Epoch: 768, Train Loss: 0.007057, Test Loss: 0.006873\n",
      "Epoch: 769, Train Loss: 0.006989, Test Loss: 0.006865\n",
      "Epoch: 770, Train Loss: 0.006960, Test Loss: 0.006880\n",
      "Epoch: 771, Train Loss: 0.006985, Test Loss: 0.006888\n",
      "Epoch: 772, Train Loss: 0.007001, Test Loss: 0.006862\n",
      "Epoch: 773, Train Loss: 0.006961, Test Loss: 0.006790\n",
      "Epoch: 774, Train Loss: 0.006904, Test Loss: 0.006791\n",
      "Epoch: 775, Train Loss: 0.006894, Test Loss: 0.006815\n",
      "Epoch: 776, Train Loss: 0.006920, Test Loss: 0.006806\n",
      "Epoch: 777, Train Loss: 0.006919, Test Loss: 0.006769\n",
      "Epoch: 778, Train Loss: 0.006873, Test Loss: 0.006722\n",
      "Epoch: 779, Train Loss: 0.006833, Test Loss: 0.006728\n",
      "Epoch: 780, Train Loss: 0.006839, Test Loss: 0.006756\n",
      "Epoch: 781, Train Loss: 0.006862, Test Loss: 0.006740\n",
      "Epoch: 782, Train Loss: 0.006852, Test Loss: 0.006703\n",
      "Epoch: 783, Train Loss: 0.006812, Test Loss: 0.006675\n",
      "Epoch: 784, Train Loss: 0.006784, Test Loss: 0.006670\n",
      "Epoch: 785, Train Loss: 0.006786, Test Loss: 0.006688\n",
      "Epoch: 786, Train Loss: 0.006795, Test Loss: 0.006673\n",
      "Epoch: 787, Train Loss: 0.006786, Test Loss: 0.006651\n",
      "Epoch: 788, Train Loss: 0.006763, Test Loss: 0.006638\n",
      "Epoch: 789, Train Loss: 0.006746, Test Loss: 0.006622\n",
      "Epoch: 790, Train Loss: 0.006740, Test Loss: 0.006630\n",
      "Epoch: 791, Train Loss: 0.006738, Test Loss: 0.006612\n",
      "Epoch: 792, Train Loss: 0.006728, Test Loss: 0.006600\n",
      "Epoch: 793, Train Loss: 0.006714, Test Loss: 0.006590\n",
      "Epoch: 794, Train Loss: 0.006702, Test Loss: 0.006576\n",
      "Epoch: 795, Train Loss: 0.006694, Test Loss: 0.006576\n",
      "Epoch: 796, Train Loss: 0.006686, Test Loss: 0.006558\n",
      "Epoch: 797, Train Loss: 0.006676, Test Loss: 0.006549\n",
      "Epoch: 798, Train Loss: 0.006664, Test Loss: 0.006541\n",
      "Epoch: 799, Train Loss: 0.006656, Test Loss: 0.006531\n",
      "Epoch: 800, Train Loss: 0.006650, Test Loss: 0.006531\n",
      "Epoch: 801, Train Loss: 0.006644, Test Loss: 0.006516\n",
      "Epoch: 802, Train Loss: 0.006635, Test Loss: 0.006508\n",
      "Epoch: 803, Train Loss: 0.006624, Test Loss: 0.006498\n",
      "Epoch: 804, Train Loss: 0.006614, Test Loss: 0.006487\n",
      "Epoch: 805, Train Loss: 0.006607, Test Loss: 0.006491\n",
      "Epoch: 806, Train Loss: 0.006604, Test Loss: 0.006481\n",
      "Epoch: 807, Train Loss: 0.006603, Test Loss: 0.006496\n",
      "Epoch: 808, Train Loss: 0.006606, Test Loss: 0.006493\n",
      "Epoch: 809, Train Loss: 0.006616, Test Loss: 0.006529\n",
      "Epoch: 810, Train Loss: 0.006639, Test Loss: 0.006565\n",
      "Epoch: 811, Train Loss: 0.006689, Test Loss: 0.006671\n",
      "Epoch: 812, Train Loss: 0.006782, Test Loss: 0.006814\n",
      "Epoch: 813, Train Loss: 0.006939, Test Loss: 0.007042\n",
      "Epoch: 814, Train Loss: 0.007160, Test Loss: 0.007251\n",
      "Epoch: 815, Train Loss: 0.007382, Test Loss: 0.007315\n",
      "Epoch: 816, Train Loss: 0.007453, Test Loss: 0.007141\n",
      "Epoch: 817, Train Loss: 0.007260, Test Loss: 0.006785\n",
      "Epoch: 818, Train Loss: 0.006927, Test Loss: 0.006673\n",
      "Epoch: 819, Train Loss: 0.006742, Test Loss: 0.006636\n",
      "Epoch: 820, Train Loss: 0.006762, Test Loss: 0.006696\n",
      "Epoch: 821, Train Loss: 0.006783, Test Loss: 0.006564\n",
      "Epoch: 822, Train Loss: 0.006689, Test Loss: 0.006492\n",
      "Epoch: 823, Train Loss: 0.006624, Test Loss: 0.006605\n",
      "Epoch: 824, Train Loss: 0.006679, Test Loss: 0.006568\n",
      "Epoch: 825, Train Loss: 0.006692, Test Loss: 0.006469\n",
      "Epoch: 826, Train Loss: 0.006563, Test Loss: 0.006353\n",
      "Epoch: 827, Train Loss: 0.006470, Test Loss: 0.006410\n",
      "Epoch: 828, Train Loss: 0.006541, Test Loss: 0.006529\n",
      "Epoch: 829, Train Loss: 0.006608, Test Loss: 0.006389\n",
      "Epoch: 830, Train Loss: 0.006518, Test Loss: 0.006302\n",
      "Epoch: 831, Train Loss: 0.006417, Test Loss: 0.006346\n",
      "Epoch: 832, Train Loss: 0.006450, Test Loss: 0.006375\n",
      "Epoch: 833, Train Loss: 0.006505, Test Loss: 0.006364\n",
      "Epoch: 834, Train Loss: 0.006461, Test Loss: 0.006270\n",
      "Epoch: 835, Train Loss: 0.006398, Test Loss: 0.006285\n",
      "Epoch: 836, Train Loss: 0.006406, Test Loss: 0.006315\n",
      "Epoch: 837, Train Loss: 0.006421, Test Loss: 0.006264\n",
      "Epoch: 838, Train Loss: 0.006388, Test Loss: 0.006249\n",
      "Epoch: 839, Train Loss: 0.006364, Test Loss: 0.006261\n",
      "Epoch: 840, Train Loss: 0.006379, Test Loss: 0.006257\n",
      "Epoch: 841, Train Loss: 0.006376, Test Loss: 0.006222\n",
      "Epoch: 842, Train Loss: 0.006339, Test Loss: 0.006204\n",
      "Epoch: 843, Train Loss: 0.006322, Test Loss: 0.006218\n",
      "Epoch: 844, Train Loss: 0.006339, Test Loss: 0.006227\n",
      "Epoch: 845, Train Loss: 0.006342, Test Loss: 0.006194\n",
      "Epoch: 846, Train Loss: 0.006313, Test Loss: 0.006172\n",
      "Epoch: 847, Train Loss: 0.006292, Test Loss: 0.006183\n",
      "Epoch: 848, Train Loss: 0.006297, Test Loss: 0.006179\n",
      "Epoch: 849, Train Loss: 0.006298, Test Loss: 0.006163\n",
      "Epoch: 850, Train Loss: 0.006283, Test Loss: 0.006161\n",
      "Epoch: 851, Train Loss: 0.006273, Test Loss: 0.006149\n",
      "Epoch: 852, Train Loss: 0.006274, Test Loss: 0.006162\n",
      "Epoch: 853, Train Loss: 0.006271, Test Loss: 0.006136\n",
      "Epoch: 854, Train Loss: 0.006258, Test Loss: 0.006138\n",
      "Epoch: 855, Train Loss: 0.006252, Test Loss: 0.006145\n",
      "Epoch: 856, Train Loss: 0.006261, Test Loss: 0.006160\n",
      "Epoch: 857, Train Loss: 0.006275, Test Loss: 0.006173\n",
      "Epoch: 858, Train Loss: 0.006292, Test Loss: 0.006217\n",
      "Epoch: 859, Train Loss: 0.006325, Test Loss: 0.006263\n",
      "Epoch: 860, Train Loss: 0.006392, Test Loss: 0.006409\n",
      "Epoch: 861, Train Loss: 0.006504, Test Loss: 0.006511\n",
      "Epoch: 862, Train Loss: 0.006652, Test Loss: 0.006726\n",
      "Epoch: 863, Train Loss: 0.006814, Test Loss: 0.006750\n",
      "Epoch: 864, Train Loss: 0.006895, Test Loss: 0.006723\n",
      "Epoch: 865, Train Loss: 0.006818, Test Loss: 0.006446\n",
      "Epoch: 866, Train Loss: 0.006573, Test Loss: 0.006233\n",
      "Epoch: 867, Train Loss: 0.006344, Test Loss: 0.006179\n",
      "Epoch: 868, Train Loss: 0.006280, Test Loss: 0.006217\n",
      "Epoch: 869, Train Loss: 0.006343, Test Loss: 0.006275\n",
      "Epoch: 870, Train Loss: 0.006375, Test Loss: 0.006192\n",
      "Epoch: 871, Train Loss: 0.006312, Test Loss: 0.006138\n",
      "Epoch: 872, Train Loss: 0.006252, Test Loss: 0.006164\n",
      "Epoch: 873, Train Loss: 0.006265, Test Loss: 0.006164\n",
      "Epoch: 874, Train Loss: 0.006287, Test Loss: 0.006140\n",
      "Epoch: 875, Train Loss: 0.006244, Test Loss: 0.006063\n",
      "Epoch: 876, Train Loss: 0.006176, Test Loss: 0.006049\n",
      "Epoch: 877, Train Loss: 0.006167, Test Loss: 0.006096\n",
      "Epoch: 878, Train Loss: 0.006197, Test Loss: 0.006072\n",
      "Epoch: 879, Train Loss: 0.006193, Test Loss: 0.006048\n",
      "Epoch: 880, Train Loss: 0.006156, Test Loss: 0.006036\n",
      "Epoch: 881, Train Loss: 0.006142, Test Loss: 0.006033\n",
      "Epoch: 882, Train Loss: 0.006152, Test Loss: 0.006036\n",
      "Epoch: 883, Train Loss: 0.006138, Test Loss: 0.005987\n",
      "Epoch: 884, Train Loss: 0.006102, Test Loss: 0.005979\n",
      "Epoch: 885, Train Loss: 0.006091, Test Loss: 0.006009\n",
      "Epoch: 886, Train Loss: 0.006112, Test Loss: 0.006001\n",
      "Epoch: 887, Train Loss: 0.006118, Test Loss: 0.005989\n",
      "Epoch: 888, Train Loss: 0.006093, Test Loss: 0.005960\n",
      "Epoch: 889, Train Loss: 0.006068, Test Loss: 0.005955\n",
      "Epoch: 890, Train Loss: 0.006068, Test Loss: 0.005972\n",
      "Epoch: 891, Train Loss: 0.006073, Test Loss: 0.005949\n",
      "Epoch: 892, Train Loss: 0.006062, Test Loss: 0.005941\n",
      "Epoch: 893, Train Loss: 0.006047, Test Loss: 0.005939\n",
      "Epoch: 894, Train Loss: 0.006044, Test Loss: 0.005936\n",
      "Epoch: 895, Train Loss: 0.006047, Test Loss: 0.005941\n",
      "Epoch: 896, Train Loss: 0.006043, Test Loss: 0.005922\n",
      "Epoch: 897, Train Loss: 0.006031, Test Loss: 0.005918\n",
      "Epoch: 898, Train Loss: 0.006024, Test Loss: 0.005918\n",
      "Epoch: 899, Train Loss: 0.006022, Test Loss: 0.005910\n",
      "Epoch: 900, Train Loss: 0.006018, Test Loss: 0.005904\n",
      "Epoch: 901, Train Loss: 0.006008, Test Loss: 0.005896\n",
      "Epoch: 902, Train Loss: 0.005999, Test Loss: 0.005889\n",
      "Epoch: 903, Train Loss: 0.005996, Test Loss: 0.005894\n",
      "Epoch: 904, Train Loss: 0.005995, Test Loss: 0.005883\n",
      "Epoch: 905, Train Loss: 0.005990, Test Loss: 0.005880\n",
      "Epoch: 906, Train Loss: 0.005983, Test Loss: 0.005877\n",
      "Epoch: 907, Train Loss: 0.005979, Test Loss: 0.005870\n",
      "Epoch: 908, Train Loss: 0.005977, Test Loss: 0.005876\n",
      "Epoch: 909, Train Loss: 0.005975, Test Loss: 0.005863\n",
      "Epoch: 910, Train Loss: 0.005970, Test Loss: 0.005865\n",
      "Epoch: 911, Train Loss: 0.005965, Test Loss: 0.005859\n",
      "Epoch: 912, Train Loss: 0.005963, Test Loss: 0.005861\n",
      "Epoch: 913, Train Loss: 0.005963, Test Loss: 0.005864\n",
      "Epoch: 914, Train Loss: 0.005965, Test Loss: 0.005867\n",
      "Epoch: 915, Train Loss: 0.005969, Test Loss: 0.005876\n",
      "Epoch: 916, Train Loss: 0.005978, Test Loss: 0.005899\n",
      "Epoch: 917, Train Loss: 0.005998, Test Loss: 0.005931\n",
      "Epoch: 918, Train Loss: 0.006035, Test Loss: 0.006004\n",
      "Epoch: 919, Train Loss: 0.006100, Test Loss: 0.006105\n",
      "Epoch: 920, Train Loss: 0.006210, Test Loss: 0.006294\n",
      "Epoch: 921, Train Loss: 0.006387, Test Loss: 0.006531\n",
      "Epoch: 922, Train Loss: 0.006638, Test Loss: 0.006829\n",
      "Epoch: 923, Train Loss: 0.006920, Test Loss: 0.006971\n",
      "Epoch: 924, Train Loss: 0.007080, Test Loss: 0.006843\n",
      "Epoch: 925, Train Loss: 0.006937, Test Loss: 0.006387\n",
      "Epoch: 926, Train Loss: 0.006497, Test Loss: 0.005998\n",
      "Epoch: 927, Train Loss: 0.006097, Test Loss: 0.005948\n",
      "Epoch: 928, Train Loss: 0.006046, Test Loss: 0.006134\n",
      "Epoch: 929, Train Loss: 0.006234, Test Loss: 0.006212\n",
      "Epoch: 930, Train Loss: 0.006304, Test Loss: 0.006030\n",
      "Epoch: 931, Train Loss: 0.006135, Test Loss: 0.005891\n",
      "Epoch: 932, Train Loss: 0.005990, Test Loss: 0.005955\n",
      "Epoch: 933, Train Loss: 0.006052, Test Loss: 0.006034\n",
      "Epoch: 934, Train Loss: 0.006134, Test Loss: 0.005947\n",
      "Epoch: 935, Train Loss: 0.006042, Test Loss: 0.005814\n",
      "Epoch: 936, Train Loss: 0.005917, Test Loss: 0.005859\n",
      "Epoch: 937, Train Loss: 0.005957, Test Loss: 0.005947\n",
      "Epoch: 938, Train Loss: 0.006044, Test Loss: 0.005889\n",
      "Epoch: 939, Train Loss: 0.005988, Test Loss: 0.005786\n",
      "Epoch: 940, Train Loss: 0.005882, Test Loss: 0.005799\n",
      "Epoch: 941, Train Loss: 0.005898, Test Loss: 0.005869\n",
      "Epoch: 942, Train Loss: 0.005965, Test Loss: 0.005838\n",
      "Epoch: 943, Train Loss: 0.005935, Test Loss: 0.005768\n",
      "Epoch: 944, Train Loss: 0.005866, Test Loss: 0.005778\n",
      "Epoch: 945, Train Loss: 0.005874, Test Loss: 0.005814\n",
      "Epoch: 946, Train Loss: 0.005910, Test Loss: 0.005790\n",
      "Epoch: 947, Train Loss: 0.005885, Test Loss: 0.005747\n",
      "Epoch: 948, Train Loss: 0.005844, Test Loss: 0.005760\n",
      "Epoch: 949, Train Loss: 0.005856, Test Loss: 0.005781\n",
      "Epoch: 950, Train Loss: 0.005876, Test Loss: 0.005757\n",
      "Epoch: 951, Train Loss: 0.005853, Test Loss: 0.005730\n",
      "Epoch: 952, Train Loss: 0.005825, Test Loss: 0.005737\n",
      "Epoch: 953, Train Loss: 0.005833, Test Loss: 0.005751\n",
      "Epoch: 954, Train Loss: 0.005846, Test Loss: 0.005737\n",
      "Epoch: 955, Train Loss: 0.005831, Test Loss: 0.005716\n",
      "Epoch: 956, Train Loss: 0.005812, Test Loss: 0.005721\n",
      "Epoch: 957, Train Loss: 0.005814, Test Loss: 0.005725\n",
      "Epoch: 958, Train Loss: 0.005819, Test Loss: 0.005713\n",
      "Epoch: 959, Train Loss: 0.005809, Test Loss: 0.005706\n",
      "Epoch: 960, Train Loss: 0.005798, Test Loss: 0.005703\n",
      "Epoch: 961, Train Loss: 0.005799, Test Loss: 0.005708\n",
      "Epoch: 962, Train Loss: 0.005800, Test Loss: 0.005697\n",
      "Epoch: 963, Train Loss: 0.005791, Test Loss: 0.005688\n",
      "Epoch: 964, Train Loss: 0.005782, Test Loss: 0.005691\n",
      "Epoch: 965, Train Loss: 0.005782, Test Loss: 0.005687\n",
      "Epoch: 966, Train Loss: 0.005783, Test Loss: 0.005686\n",
      "Epoch: 967, Train Loss: 0.005777, Test Loss: 0.005675\n",
      "Epoch: 968, Train Loss: 0.005769, Test Loss: 0.005673\n",
      "Epoch: 969, Train Loss: 0.005766, Test Loss: 0.005675\n",
      "Epoch: 970, Train Loss: 0.005766, Test Loss: 0.005668\n",
      "Epoch: 971, Train Loss: 0.005762, Test Loss: 0.005666\n",
      "Epoch: 972, Train Loss: 0.005757, Test Loss: 0.005660\n",
      "Epoch: 973, Train Loss: 0.005753, Test Loss: 0.005659\n",
      "Epoch: 974, Train Loss: 0.005751, Test Loss: 0.005656\n",
      "Epoch: 975, Train Loss: 0.005748, Test Loss: 0.005651\n",
      "Epoch: 976, Train Loss: 0.005743, Test Loss: 0.005649\n",
      "Epoch: 977, Train Loss: 0.005740, Test Loss: 0.005645\n",
      "Epoch: 978, Train Loss: 0.005737, Test Loss: 0.005644\n",
      "Epoch: 979, Train Loss: 0.005734, Test Loss: 0.005639\n",
      "Epoch: 980, Train Loss: 0.005731, Test Loss: 0.005636\n",
      "Epoch: 981, Train Loss: 0.005727, Test Loss: 0.005632\n",
      "Epoch: 982, Train Loss: 0.005724, Test Loss: 0.005631\n",
      "Epoch: 983, Train Loss: 0.005721, Test Loss: 0.005627\n",
      "Epoch: 984, Train Loss: 0.005718, Test Loss: 0.005624\n",
      "Epoch: 985, Train Loss: 0.005714, Test Loss: 0.005621\n",
      "Epoch: 986, Train Loss: 0.005711, Test Loss: 0.005617\n",
      "Epoch: 987, Train Loss: 0.005708, Test Loss: 0.005616\n",
      "Epoch: 988, Train Loss: 0.005705, Test Loss: 0.005611\n",
      "Epoch: 989, Train Loss: 0.005702, Test Loss: 0.005609\n",
      "Epoch: 990, Train Loss: 0.005699, Test Loss: 0.005605\n",
      "Epoch: 991, Train Loss: 0.005696, Test Loss: 0.005604\n",
      "Epoch: 992, Train Loss: 0.005693, Test Loss: 0.005600\n",
      "Epoch: 993, Train Loss: 0.005690, Test Loss: 0.005600\n",
      "Epoch: 994, Train Loss: 0.005687, Test Loss: 0.005594\n",
      "Epoch: 995, Train Loss: 0.005685, Test Loss: 0.005596\n",
      "Epoch: 996, Train Loss: 0.005683, Test Loss: 0.005590\n",
      "Epoch: 997, Train Loss: 0.005682, Test Loss: 0.005597\n",
      "Epoch: 998, Train Loss: 0.005682, Test Loss: 0.005592\n",
      "Epoch: 999, Train Loss: 0.005685, Test Loss: 0.005612\n",
      "Epoch: 1000, Train Loss: 0.005693, Test Loss: 0.005618\n",
      "Epoch: 1001, Train Loss: 0.005712, Test Loss: 0.005676\n",
      "Epoch: 1002, Train Loss: 0.005750, Test Loss: 0.005728\n",
      "Epoch: 1003, Train Loss: 0.005822, Test Loss: 0.005890\n",
      "Epoch: 1004, Train Loss: 0.005947, Test Loss: 0.006043\n",
      "Epoch: 1005, Train Loss: 0.006131, Test Loss: 0.006294\n",
      "Epoch: 1006, Train Loss: 0.006326, Test Loss: 0.006316\n",
      "Epoch: 1007, Train Loss: 0.006397, Test Loss: 0.006194\n",
      "Epoch: 1008, Train Loss: 0.006240, Test Loss: 0.005911\n",
      "Epoch: 1009, Train Loss: 0.005994, Test Loss: 0.005862\n",
      "Epoch: 1010, Train Loss: 0.005954, Test Loss: 0.006097\n",
      "Epoch: 1011, Train Loss: 0.006147, Test Loss: 0.006170\n",
      "Epoch: 1012, Train Loss: 0.006284, Test Loss: 0.006123\n",
      "Epoch: 1013, Train Loss: 0.006167, Test Loss: 0.005854\n",
      "Epoch: 1014, Train Loss: 0.005961, Test Loss: 0.005806\n",
      "Epoch: 1015, Train Loss: 0.005866, Test Loss: 0.005741\n",
      "Epoch: 1016, Train Loss: 0.005813, Test Loss: 0.005652\n",
      "Epoch: 1017, Train Loss: 0.005748, Test Loss: 0.005708\n",
      "Epoch: 1018, Train Loss: 0.005779, Test Loss: 0.005781\n",
      "Epoch: 1019, Train Loss: 0.005872, Test Loss: 0.005766\n",
      "Epoch: 1020, Train Loss: 0.005831, Test Loss: 0.005600\n",
      "Epoch: 1021, Train Loss: 0.005690, Test Loss: 0.005579\n",
      "Epoch: 1022, Train Loss: 0.005661, Test Loss: 0.005649\n",
      "Epoch: 1023, Train Loss: 0.005726, Test Loss: 0.005638\n",
      "Epoch: 1024, Train Loss: 0.005730, Test Loss: 0.005620\n",
      "Epoch: 1025, Train Loss: 0.005690, Test Loss: 0.005582\n",
      "Epoch: 1026, Train Loss: 0.005676, Test Loss: 0.005579\n",
      "Epoch: 1027, Train Loss: 0.005659, Test Loss: 0.005559\n",
      "Epoch: 1028, Train Loss: 0.005638, Test Loss: 0.005558\n",
      "Epoch: 1029, Train Loss: 0.005653, Test Loss: 0.005589\n",
      "Epoch: 1030, Train Loss: 0.005659, Test Loss: 0.005534\n",
      "Epoch: 1031, Train Loss: 0.005625, Test Loss: 0.005522\n",
      "Epoch: 1032, Train Loss: 0.005609, Test Loss: 0.005551\n",
      "Epoch: 1033, Train Loss: 0.005626, Test Loss: 0.005527\n",
      "Epoch: 1034, Train Loss: 0.005619, Test Loss: 0.005515\n",
      "Epoch: 1035, Train Loss: 0.005595, Test Loss: 0.005516\n",
      "Epoch: 1036, Train Loss: 0.005600, Test Loss: 0.005520\n",
      "Epoch: 1037, Train Loss: 0.005607, Test Loss: 0.005510\n",
      "Epoch: 1038, Train Loss: 0.005589, Test Loss: 0.005490\n",
      "Epoch: 1039, Train Loss: 0.005577, Test Loss: 0.005500\n",
      "Epoch: 1040, Train Loss: 0.005583, Test Loss: 0.005500\n",
      "Epoch: 1041, Train Loss: 0.005581, Test Loss: 0.005487\n",
      "Epoch: 1042, Train Loss: 0.005571, Test Loss: 0.005487\n",
      "Epoch: 1043, Train Loss: 0.005570, Test Loss: 0.005487\n",
      "Epoch: 1044, Train Loss: 0.005569, Test Loss: 0.005474\n",
      "Epoch: 1045, Train Loss: 0.005557, Test Loss: 0.005468\n",
      "Epoch: 1046, Train Loss: 0.005552, Test Loss: 0.005476\n",
      "Epoch: 1047, Train Loss: 0.005557, Test Loss: 0.005473\n",
      "Epoch: 1048, Train Loss: 0.005556, Test Loss: 0.005463\n",
      "Epoch: 1049, Train Loss: 0.005547, Test Loss: 0.005462\n",
      "Epoch: 1050, Train Loss: 0.005542, Test Loss: 0.005456\n",
      "Epoch: 1051, Train Loss: 0.005541, Test Loss: 0.005457\n",
      "Epoch: 1052, Train Loss: 0.005538, Test Loss: 0.005455\n",
      "Epoch: 1053, Train Loss: 0.005537, Test Loss: 0.005455\n",
      "Epoch: 1054, Train Loss: 0.005538, Test Loss: 0.005458\n",
      "Epoch: 1055, Train Loss: 0.005538, Test Loss: 0.005455\n",
      "Epoch: 1056, Train Loss: 0.005538, Test Loss: 0.005459\n",
      "Epoch: 1057, Train Loss: 0.005543, Test Loss: 0.005478\n",
      "Epoch: 1058, Train Loss: 0.005557, Test Loss: 0.005496\n",
      "Epoch: 1059, Train Loss: 0.005582, Test Loss: 0.005547\n",
      "Epoch: 1060, Train Loss: 0.005627, Test Loss: 0.005621\n",
      "Epoch: 1061, Train Loss: 0.005707, Test Loss: 0.005760\n",
      "Epoch: 1062, Train Loss: 0.005839, Test Loss: 0.005950\n",
      "Epoch: 1063, Train Loss: 0.006036, Test Loss: 0.006207\n",
      "Epoch: 1064, Train Loss: 0.006284, Test Loss: 0.006387\n",
      "Epoch: 1065, Train Loss: 0.006475, Test Loss: 0.006379\n",
      "Epoch: 1066, Train Loss: 0.006454, Test Loss: 0.006062\n",
      "Epoch: 1067, Train Loss: 0.006147, Test Loss: 0.005703\n",
      "Epoch: 1068, Train Loss: 0.005780, Test Loss: 0.005555\n",
      "Epoch: 1069, Train Loss: 0.005632, Test Loss: 0.005627\n",
      "Epoch: 1070, Train Loss: 0.005712, Test Loss: 0.005715\n",
      "Epoch: 1071, Train Loss: 0.005792, Test Loss: 0.005638\n",
      "Epoch: 1072, Train Loss: 0.005728, Test Loss: 0.005560\n",
      "Epoch: 1073, Train Loss: 0.005638, Test Loss: 0.005572\n",
      "Epoch: 1074, Train Loss: 0.005648, Test Loss: 0.005596\n",
      "Epoch: 1075, Train Loss: 0.005681, Test Loss: 0.005544\n",
      "Epoch: 1076, Train Loss: 0.005618, Test Loss: 0.005442\n",
      "Epoch: 1077, Train Loss: 0.005529, Test Loss: 0.005468\n",
      "Epoch: 1078, Train Loss: 0.005546, Test Loss: 0.005536\n",
      "Epoch: 1079, Train Loss: 0.005613, Test Loss: 0.005509\n",
      "Epoch: 1080, Train Loss: 0.005594, Test Loss: 0.005440\n",
      "Epoch: 1081, Train Loss: 0.005512, Test Loss: 0.005405\n",
      "Epoch: 1082, Train Loss: 0.005488, Test Loss: 0.005450\n",
      "Epoch: 1083, Train Loss: 0.005528, Test Loss: 0.005463\n",
      "Epoch: 1084, Train Loss: 0.005538, Test Loss: 0.005420\n",
      "Epoch: 1085, Train Loss: 0.005503, Test Loss: 0.005412\n",
      "Epoch: 1086, Train Loss: 0.005483, Test Loss: 0.005407\n",
      "Epoch: 1087, Train Loss: 0.005488, Test Loss: 0.005404\n",
      "Epoch: 1088, Train Loss: 0.005484, Test Loss: 0.005396\n",
      "Epoch: 1089, Train Loss: 0.005471, Test Loss: 0.005391\n",
      "Epoch: 1090, Train Loss: 0.005472, Test Loss: 0.005402\n",
      "Epoch: 1091, Train Loss: 0.005476, Test Loss: 0.005381\n",
      "Epoch: 1092, Train Loss: 0.005460, Test Loss: 0.005363\n",
      "Epoch: 1093, Train Loss: 0.005442, Test Loss: 0.005371\n",
      "Epoch: 1094, Train Loss: 0.005445, Test Loss: 0.005376\n",
      "Epoch: 1095, Train Loss: 0.005456, Test Loss: 0.005374\n",
      "Epoch: 1096, Train Loss: 0.005449, Test Loss: 0.005355\n",
      "Epoch: 1097, Train Loss: 0.005433, Test Loss: 0.005348\n",
      "Epoch: 1098, Train Loss: 0.005427, Test Loss: 0.005355\n",
      "Epoch: 1099, Train Loss: 0.005430, Test Loss: 0.005351\n",
      "Epoch: 1100, Train Loss: 0.005430, Test Loss: 0.005347\n",
      "Epoch: 1101, Train Loss: 0.005423, Test Loss: 0.005342\n",
      "Epoch: 1102, Train Loss: 0.005419, Test Loss: 0.005339\n",
      "Epoch: 1103, Train Loss: 0.005416, Test Loss: 0.005337\n",
      "Epoch: 1104, Train Loss: 0.005412, Test Loss: 0.005329\n",
      "Epoch: 1105, Train Loss: 0.005407, Test Loss: 0.005329\n",
      "Epoch: 1106, Train Loss: 0.005405, Test Loss: 0.005329\n",
      "Epoch: 1107, Train Loss: 0.005405, Test Loss: 0.005326\n",
      "Epoch: 1108, Train Loss: 0.005403, Test Loss: 0.005321\n",
      "Epoch: 1109, Train Loss: 0.005396, Test Loss: 0.005315\n",
      "Epoch: 1110, Train Loss: 0.005391, Test Loss: 0.005314\n",
      "Epoch: 1111, Train Loss: 0.005389, Test Loss: 0.005314\n",
      "Epoch: 1112, Train Loss: 0.005389, Test Loss: 0.005311\n",
      "Epoch: 1113, Train Loss: 0.005386, Test Loss: 0.005308\n",
      "Epoch: 1114, Train Loss: 0.005382, Test Loss: 0.005304\n",
      "Epoch: 1115, Train Loss: 0.005379, Test Loss: 0.005302\n",
      "Epoch: 1116, Train Loss: 0.005377, Test Loss: 0.005300\n",
      "Epoch: 1117, Train Loss: 0.005374, Test Loss: 0.005296\n",
      "Epoch: 1118, Train Loss: 0.005371, Test Loss: 0.005294\n",
      "Epoch: 1119, Train Loss: 0.005367, Test Loss: 0.005291\n",
      "Epoch: 1120, Train Loss: 0.005365, Test Loss: 0.005290\n",
      "Epoch: 1121, Train Loss: 0.005363, Test Loss: 0.005288\n",
      "Epoch: 1122, Train Loss: 0.005361, Test Loss: 0.005284\n",
      "Epoch: 1123, Train Loss: 0.005358, Test Loss: 0.005282\n",
      "Epoch: 1124, Train Loss: 0.005354, Test Loss: 0.005279\n",
      "Epoch: 1125, Train Loss: 0.005352, Test Loss: 0.005277\n",
      "Epoch: 1126, Train Loss: 0.005350, Test Loss: 0.005275\n",
      "Epoch: 1127, Train Loss: 0.005347, Test Loss: 0.005271\n",
      "Epoch: 1128, Train Loss: 0.005344, Test Loss: 0.005269\n",
      "Epoch: 1129, Train Loss: 0.005341, Test Loss: 0.005266\n",
      "Epoch: 1130, Train Loss: 0.005339, Test Loss: 0.005265\n",
      "Epoch: 1131, Train Loss: 0.005337, Test Loss: 0.005263\n",
      "Epoch: 1132, Train Loss: 0.005334, Test Loss: 0.005260\n",
      "Epoch: 1133, Train Loss: 0.005332, Test Loss: 0.005258\n",
      "Epoch: 1134, Train Loss: 0.005329, Test Loss: 0.005255\n",
      "Epoch: 1135, Train Loss: 0.005326, Test Loss: 0.005253\n",
      "Epoch: 1136, Train Loss: 0.005324, Test Loss: 0.005250\n",
      "Epoch: 1137, Train Loss: 0.005321, Test Loss: 0.005248\n",
      "Epoch: 1138, Train Loss: 0.005318, Test Loss: 0.005245\n",
      "Epoch: 1139, Train Loss: 0.005316, Test Loss: 0.005243\n",
      "Epoch: 1140, Train Loss: 0.005313, Test Loss: 0.005240\n",
      "Epoch: 1141, Train Loss: 0.005310, Test Loss: 0.005238\n",
      "Epoch: 1142, Train Loss: 0.005308, Test Loss: 0.005236\n",
      "Epoch: 1143, Train Loss: 0.005306, Test Loss: 0.005234\n",
      "Epoch: 1144, Train Loss: 0.005303, Test Loss: 0.005231\n",
      "Epoch: 1145, Train Loss: 0.005301, Test Loss: 0.005230\n",
      "Epoch: 1146, Train Loss: 0.005299, Test Loss: 0.005229\n",
      "Epoch: 1147, Train Loss: 0.005299, Test Loss: 0.005233\n",
      "Epoch: 1148, Train Loss: 0.005301, Test Loss: 0.005238\n",
      "Epoch: 1149, Train Loss: 0.005308, Test Loss: 0.005263\n",
      "Epoch: 1150, Train Loss: 0.005329, Test Loss: 0.005307\n",
      "Epoch: 1151, Train Loss: 0.005378, Test Loss: 0.005430\n",
      "Epoch: 1152, Train Loss: 0.005490, Test Loss: 0.005656\n",
      "Epoch: 1153, Train Loss: 0.005727, Test Loss: 0.006105\n",
      "Epoch: 1154, Train Loss: 0.006151, Test Loss: 0.006585\n",
      "Epoch: 1155, Train Loss: 0.006655, Test Loss: 0.006775\n",
      "Epoch: 1156, Train Loss: 0.006809, Test Loss: 0.006149\n",
      "Epoch: 1157, Train Loss: 0.006237, Test Loss: 0.005576\n",
      "Epoch: 1158, Train Loss: 0.005628, Test Loss: 0.005634\n",
      "Epoch: 1159, Train Loss: 0.005708, Test Loss: 0.005880\n",
      "Epoch: 1160, Train Loss: 0.005946, Test Loss: 0.005652\n",
      "Epoch: 1161, Train Loss: 0.005704, Test Loss: 0.005415\n",
      "Epoch: 1162, Train Loss: 0.005501, Test Loss: 0.005589\n",
      "Epoch: 1163, Train Loss: 0.005639, Test Loss: 0.005497\n",
      "Epoch: 1164, Train Loss: 0.005561, Test Loss: 0.005333\n",
      "Epoch: 1165, Train Loss: 0.005414, Test Loss: 0.005500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rel_l2_err\u001b[38;5;241m=\u001b[39m\u001b[43mstatic_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_br_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_br_geo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_tr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 187\u001b[0m, in \u001b[0;36mstatic_main\u001b[0;34m(model, epochs, device)\u001b[0m\n\u001b[1;32m    182\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# elif epoch == epochs - 1000:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m#     optimizer = torch.optim.LBFGS(model.parameters())\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m## Training\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_u_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_g_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m## Testing\u001b[39;00m\n\u001b[1;32m    189\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m to_numpy(model\u001b[38;5;241m.\u001b[39mforward(f_u_test_tensor, f_g_test_tensor, xt_tensor)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,num_snap,num_pts))\n",
      "Cell \u001b[0;32mIn[36], line 173\u001b[0m, in \u001b[0;36mstatic_main.<locals>.train\u001b[0;34m(epoch, f_u, f_g, x, y)\u001b[0m\n\u001b[1;32m    171\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m--> 173\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[36], line 169\u001b[0m, in \u001b[0;36mstatic_main.<locals>.train.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    167\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(f_u, f_g, x)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,num_snap,num_pts)\n\u001b[0;32m--> 169\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    170\u001b[0m train_loss[epoch, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    171\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rel_l2_err=static_main(opnn(dim_br_u, dim_br_geo, dim_tr), 10000, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ReactionDiffusion/plot_data/rel_l2_test.txt\", rel_l2_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.559405435375966\n"
     ]
    }
   ],
   "source": [
    "print(rel_l2_err.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "font_path = '/root/calibrili.ttf'\n",
    "font_prop = FontProperties(fname=font_path)\n",
    "\n",
    "custom_colors = {'DIMON': '#0057B7', 'Ours': '#A52A2A'}\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"加载并转换文本文件中的科学计数法数据\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [float(line.strip()) for line in lines]\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path_group1 = 'ReactionDiffusion/plot_data/rel_l2_5_old.txt'  \n",
    "file_path_group2 = 'ReactionDiffusion/plot_data/rel_l2_5_ours.txt' \n",
    "\n",
    "\n",
    "data_group1 = load_data(file_path_group1)\n",
    "data_group2 = load_data(file_path_group2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_group1 = pd.DataFrame(data_group1, columns=['Value'])\n",
    "df_group1['Group'] = 'DIMON'\n",
    "\n",
    "df_group2 = pd.DataFrame(data_group2, columns=['Value'])\n",
    "df_group2['Group'] = 'Ours'\n",
    "\n",
    "\n",
    "df_combined = pd.concat([df_group1, df_group2], ignore_index=True)\n",
    "\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df_combined, x='Value', hue='Group', fill=True, common_norm=False, palette=custom_colors, common_grid=True)\n",
    "\n",
    "mean_group1 = df_group1['Value'].mean()\n",
    "mean_group2 = df_group2['Value'].mean()\n",
    "\n",
    "\n",
    "kde_dimon = stats.gaussian_kde(df_group1['Value'])\n",
    "kde_ours = stats.gaussian_kde(df_group2['Value'])\n",
    "\n",
    "\n",
    "x = np.linspace(min(df_combined['Value']), max(df_combined['Value']), 1000)\n",
    "\n",
    "\n",
    "y_mean_group1 = kde_dimon(mean_group1)[0]\n",
    "y_mean_group2 = kde_ours(mean_group2)[0]\n",
    "\n",
    "\n",
    "plt.plot([mean_group1, mean_group1], [0, y_mean_group1], linestyle='-', color='black', linewidth=2)\n",
    "plt.plot([mean_group2, mean_group2], [0, y_mean_group2], linestyle='-', color='red', linewidth=2)\n",
    "\n",
    "\n",
    "#plt.xlabel('Rel. L2 Err. Distribution (n=600)', fontproperties=font_prop)\n",
    "plt.xlabel('')\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([0.0, 0.1, 0.2, 0.3])\n",
    "\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_fontproperties(font_prop)\n",
    "\n",
    "\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.tick_params(axis='x', labelsize=13)\n",
    "\n",
    "sns.despine(left=True)\n",
    "\n",
    "\n",
    "legend = plt.legend(prop=font_prop)\n",
    "\n",
    "\n",
    "#plt.title('Your Title Here', fontproperties=font_prop, fontsize=16)  # 这里的16是字体大小，可以根据需要调整\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5old 0.04247022579566379\n",
    "5ours 0.03780566521717164\n",
    "\n",
    "4ours 0.05042636890934997\n",
    "4old 0.05684620483237866\n",
    "\n",
    "2ours 0.023618453823029954\n",
    "2old 0.04056565620528469\n",
    "\n",
    "3old 0.08626626184004514\n",
    "3ours 0.06632727673757731"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
